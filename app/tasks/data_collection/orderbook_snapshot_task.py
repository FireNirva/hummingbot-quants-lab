"""
è®¢å•ç°¿å¿«ç…§é‡‡é›†ä»»åŠ¡

åŠŸèƒ½ï¼š
- å®šæœŸé‡‡é›†äº¤æ˜“æ‰€è®¢å•ç°¿å¿«ç…§
- æ”¯æŒå¤šä¸ªäº¤æ˜“å¯¹
- è‡ªåŠ¨å­˜å‚¨ä¸º Parquet æ ¼å¼
- ä¸ quants-lab ç°æœ‰æ¶æ„å®Œå…¨å…¼å®¹

ä½œè€…ï¼šAlice
æ—¥æœŸï¼š2025-11-15
"""

import asyncio
import logging
from datetime import datetime, timezone
from typing import Any, Dict, Optional

import aiohttp
import pandas as pd

from core.data_sources import CLOBDataSource
from core.data_paths import data_paths
from core.tasks import BaseTask, TaskContext

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class OrderBookSnapshotTask(BaseTask):
    """
    è®¢å•ç°¿å¿«ç…§é‡‡é›†ä»»åŠ¡
    
    é…ç½®ç¤ºä¾‹ï¼š
    ```yaml
    tasks:
      orderbook_snapshot:
        enabled: true
        task_class: app.tasks.data_collection.orderbook_snapshot_task.OrderBookSnapshotTask
        
        schedule:
          type: frequency
          frequency_minutes: 1  # æ¯åˆ†é’Ÿé‡‡é›†ä¸€æ¬¡
        
        config:
          connector_name: "gate_io"
          trading_pairs:
            - "IRON-USDT"
            - "VIRTUAL-USDT"
            - "AERO-USDT"
          depth_limit: 100  # è®¢å•ç°¿æ·±åº¦ï¼ˆæ¡£ä½æ•°ï¼‰
    ```
    """
    
    def __init__(self, config):
        super().__init__(config)
        
        # é…ç½®å‚æ•°
        task_config = self.config.config
        self.connector_name = task_config["connector_name"]
        self.trading_pairs = task_config.get("trading_pairs", [])
        self.depth_limit = task_config.get("depth_limit", 100)
        
        # åˆå§‹åŒ–æ•°æ®æº
        self.clob = CLOBDataSource()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir = data_paths.raw_dir / "orderbook_snapshots"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("OrderBookSnapshotTask initialized")
        logger.info(f"  Connector: {self.connector_name}")
        logger.info(f"  Trading pairs: {len(self.trading_pairs)}")
        logger.info(f"  Depth limit: {self.depth_limit}")
    
    async def setup(self, context: TaskContext) -> None:
        """ä»»åŠ¡å¯åŠ¨å‰çš„è®¾ç½®"""
        try:
            await super().setup(context)
            
            # éªŒè¯å¿…è¦å‚æ•°
            if not self.connector_name:
                raise RuntimeError("connector_name not configured")
            
            if not self.trading_pairs:
                raise RuntimeError("trading_pairs not configured")
            
            # è·å–è¿æ¥å™¨
            try:
                self.connector = self.clob.get_connector(self.connector_name)
                logger.info(f"Connector '{self.connector_name}' initialized successfully")
            except Exception as e:
                raise RuntimeError(f"Failed to initialize connector '{self.connector_name}': {e}")
            
        except Exception as e:
            logger.error(f"Setup failed: {e}")
            raise
    
    async def execute(self, context: TaskContext) -> Dict[str, Any]:
        """
        ä¸»æ‰§è¡Œé€»è¾‘ï¼šé‡‡é›†è®¢å•ç°¿å¿«ç…§
        
        å¹¶å‘æ§åˆ¶ï¼šä½¿ç”¨ Semaphore é™åˆ¶åŒæ—¶è¯·æ±‚æ•°ï¼Œé¿å…è§¦å‘ API é™æµ
        """
        start_time = datetime.now(timezone.utc)
        logger.info(f"Starting orderbook snapshot collection for {len(self.trading_pairs)} pairs")
        
        try:
            stats = {
                "pairs_processed": 0,
                "pairs_total": len(self.trading_pairs),
                "snapshots_collected": 0,
                "errors": 0,
                "start_time": start_time.isoformat(),
            }
            
            # å¹¶å‘æ§åˆ¶ï¼šé™åˆ¶åŒæ—¶è¯·æ±‚æ•°ï¼ˆé¿å…è§¦å‘ Gate.io çš„10ä¸ªå¹¶å‘è¿æ¥é™åˆ¶ï¼‰
            MAX_CONCURRENT = 8  # å®‰å…¨å€¼ï¼šå°äºé™åˆ¶ï¼Œç•™æœ‰ä½™åœ°
            semaphore = asyncio.Semaphore(MAX_CONCURRENT)
            
            async def collect_with_limit(pair):
                """å¸¦å¹¶å‘é™åˆ¶çš„é‡‡é›†åŒ…è£…å™¨"""
                async with semaphore:
                    # ç§»é™¤å»¶è¿Ÿä»¥å®ç°æ›´ç²¾ç¡®çš„ 5 ç§’é‡‡é›†é—´éš”
                    # Semaphore å·²ç»æä¾›äº†è¶³å¤Ÿçš„å¹¶å‘æ§åˆ¶
                    return await self._collect_orderbook_snapshot(pair)
            
            logger.info(f"Using concurrent limit: {MAX_CONCURRENT}")
            
            # å¹¶å‘é‡‡é›†æ‰€æœ‰äº¤æ˜“å¯¹çš„è®¢å•ç°¿ï¼ˆå—å¹¶å‘æ•°é™åˆ¶ï¼‰
            tasks = [
                collect_with_limit(pair)
                for pair in self.trading_pairs
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»Ÿè®¡ç»“æœ
            for result in results:
                if isinstance(result, Exception):
                    stats["errors"] += 1
                    logger.error(f"Error collecting snapshot: {result}")
                elif result:
                    stats["snapshots_collected"] += 1
                    stats["pairs_processed"] += 1
            
            # è®¡ç®—æ‰§è¡Œæ—¶é•¿
            end_time = datetime.now(timezone.utc)
            stats["end_time"] = end_time.isoformat()
            stats["duration_seconds"] = (end_time - start_time).total_seconds()
            
            logger.info(f"Orderbook snapshot collection completed: {stats['snapshots_collected']}/{stats['pairs_total']} successful")
            
            return {
                "success": True,
                "stats": stats
            }
            
        except Exception as e:
            logger.error(f"Execute failed: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _collect_orderbook_snapshot(self, trading_pair: str) -> bool:
        """
        é‡‡é›†å•ä¸ªäº¤æ˜“å¯¹çš„è®¢å•ç°¿å¿«ç…§ï¼ˆåŒ…å« update_idï¼‰
        
        Args:
            trading_pair: äº¤æ˜“å¯¹ï¼ˆå¦‚ "IRON-USDT"ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.debug(f"Collecting orderbook for {trading_pair}")
            
            # æ ¼å¼åŒ–äº¤æ˜“å¯¹åç§°ï¼ˆä¸åŒäº¤æ˜“æ‰€æ ¼å¼ä¸åŒï¼‰
            if self.connector_name == "gate_io":
                formatted_pair = trading_pair.replace('-', '_')  # Gate.io: BTC_USDT
            elif self.connector_name == "mexc":
                formatted_pair = trading_pair.replace('-', '')   # MEXC: BTCUSDT
            else:
                formatted_pair = trading_pair.replace('-', '_')  # é»˜è®¤ä½¿ç”¨ä¸‹åˆ’çº¿
            
            # ğŸ†• æ ¹æ®äº¤æ˜“æ‰€ç±»å‹è°ƒç”¨ç›¸åº”çš„ API
            orderbook_data = await self._fetch_orderbook(formatted_pair)
            
            if not orderbook_data:
                logger.error(f"Failed to fetch orderbook for {trading_pair}")
                return False
            
            # æå–æ•°æ®
            timestamp = datetime.now(timezone.utc)
            update_id = orderbook_data.get('id')  # ğŸ†• Update ID (sequence_number)
            bids = orderbook_data.get('bids', [])
            asks = orderbook_data.get('asks', [])
            
            # é™åˆ¶æ·±åº¦
            bids = bids[:self.depth_limit]
            asks = asks[:self.depth_limit]
            
            # æ„å»ºæ•°æ®ç»“æ„ï¼ˆæ·»åŠ  update_idï¼‰
            snapshot_data = {
                'timestamp': timestamp,
                'update_id': update_id,  # ğŸ†• æ·»åŠ  update_id å­—æ®µ
                'exchange': self.connector_name,
                'trading_pair': trading_pair,
                'best_bid_price': float(bids[0][0]) if bids else None,
                'best_bid_amount': float(bids[0][1]) if bids else None,
                'best_ask_price': float(asks[0][0]) if asks else None,
                'best_ask_amount': float(asks[0][1]) if asks else None,
                'bid_prices': [float(b[0]) for b in bids],
                'bid_amounts': [float(b[1]) for b in bids],
                'ask_prices': [float(a[0]) for a in asks],
                'ask_amounts': [float(a[1]) for a in asks],
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            await self._save_snapshot(snapshot_data)
            
            logger.debug(f"âœ… {trading_pair}: Collected with update_id={update_id}, {len(bids)} bids, {len(asks)} asks")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ {trading_pair}: Failed to collect orderbook: {e}")
            return False
    
    async def _fetch_orderbook(self, formatted_pair: str) -> Optional[Dict]:
        """
        æ ¹æ®äº¤æ˜“æ‰€ç±»å‹è°ƒç”¨ç›¸åº”çš„ API è·å–è®¢å•ç°¿
        
        Args:
            formatted_pair: æ ¼å¼åŒ–åçš„äº¤æ˜“å¯¹ï¼ˆå¦‚ "BTC_USDT"ï¼‰
        
        Returns:
            è®¢å•ç°¿æ•°æ®å­—å…¸ï¼ŒåŒ…å« 'id', 'bids', 'asks' ç­‰å­—æ®µ
        """
        if self.connector_name == "gate_io":
            return await self._fetch_gateio_orderbook(formatted_pair)
        elif self.connector_name == "mexc":
            return await self._fetch_mexc_orderbook(formatted_pair)
        else:
            logger.error(f"Unsupported exchange: {self.connector_name}")
            return None
    
    async def _fetch_gateio_orderbook(self, formatted_pair: str) -> Optional[Dict]:
        """
        ç›´æ¥è°ƒç”¨ Gate.io API è·å–è®¢å•ç°¿ï¼ˆåŒ…å« update_idï¼‰
        
        Args:
            formatted_pair: æ ¼å¼åŒ–åçš„äº¤æ˜“å¯¹ï¼ˆå¦‚ "BTC_USDT"ï¼‰
        
        Returns:
            è®¢å•ç°¿æ•°æ®å­—å…¸ï¼ŒåŒ…å« 'id', 'bids', 'asks' ç­‰å­—æ®µ
        """
        try:
            url = "https://api.gateio.ws/api/v4/spot/order_book"
            params = {
                "currency_pair": formatted_pair,
                "limit": self.depth_limit,
                "with_id": "true"  # ğŸ”‘ å…³é”®å‚æ•°ï¼šè¿”å› update_id
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=5)) as response:
                    if response.status != 200:
                        text = await response.text()
                        logger.error(f"Gate.io API error {response.status}: {text}")
                        return None
                    
                    data = await response.json()
                    
                    # éªŒè¯è¿”å›æ•°æ®
                    if 'id' not in data:
                        logger.warning(f"No 'id' field in response for {formatted_pair}")
                    
                    return data
                    
        except asyncio.TimeoutError:
            logger.error(f"Timeout fetching orderbook for {formatted_pair}")
            return None
        except Exception as e:
            logger.error(f"Error fetching orderbook for {formatted_pair}: {e}")
            return None
    
    async def _fetch_mexc_orderbook(self, formatted_pair: str) -> Optional[Dict]:
        """
        ç›´æ¥è°ƒç”¨ MEXC API è·å–è®¢å•ç°¿ï¼ˆåŒ…å« update_idï¼‰
        
        MEXC API æ–‡æ¡£: https://mexcdevelop.github.io/apidocs/spot_v3_en/#order-book
        
        Args:
            formatted_pair: æ ¼å¼åŒ–åçš„äº¤æ˜“å¯¹ï¼ˆå¦‚ "BTCUSDT"ï¼ŒMEXC ä¸ä½¿ç”¨ä¸‹åˆ’çº¿ï¼‰
        
        Returns:
            è®¢å•ç°¿æ•°æ®å­—å…¸ï¼Œç»Ÿä¸€æ ¼å¼ä¸ Gate.io ä¸€è‡´
        """
        try:
            url = "https://api.mexc.com/api/v3/depth"
            params = {
                "symbol": formatted_pair,
                "limit": self.depth_limit
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=5)) as response:
                    if response.status != 200:
                        text = await response.text()
                        logger.error(f"MEXC API error {response.status}: {text}")
                        return None
                    
                    data = await response.json()
                    
                    # MEXC è¿”å›æ ¼å¼ï¼š
                    # {
                    #   "lastUpdateId": 548631456,  # ç›¸å½“äº Gate.io çš„ 'id'
                    #   "bids": [["19549.73", "0.342"], ...],
                    #   "asks": [["19549.74", "0.5"], ...]
                    # }
                    
                    # ç»Ÿä¸€æ ¼å¼ä¸º Gate.io é£æ ¼
                    normalized_data = {
                        'id': data.get('lastUpdateId'),  # MEXC çš„åºåˆ—å·
                        'bids': data.get('bids', []),
                        'asks': data.get('asks', [])
                    }
                    
                    if 'lastUpdateId' not in data:
                        logger.warning(f"No 'lastUpdateId' field in MEXC response for {formatted_pair}")
                    
                    return normalized_data
                    
        except asyncio.TimeoutError:
            logger.error(f"Timeout fetching orderbook for {formatted_pair}")
            return None
        except Exception as e:
            logger.error(f"Error fetching orderbook for {formatted_pair}: {e}")
            return None
    
    async def _save_snapshot(self, snapshot_data: Dict):
        """
        ä¿å­˜è®¢å•ç°¿å¿«ç…§åˆ° Parquet æ–‡ä»¶
        
        ç­–ç•¥ï¼š
        - æ¯å¤©ä¸€ä¸ªæ–‡ä»¶ï¼ˆæŒ‰æ—¥æœŸåˆ†åŒºï¼‰
        - å¢é‡è¿½åŠ æ¨¡å¼
        - ä½¿ç”¨ Parquet å‹ç¼©å­˜å‚¨
        """
        try:
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆæŒ‰æ—¥æœŸåˆ†åŒºï¼‰
            date_str = snapshot_data['timestamp'].strftime('%Y%m%d')
            filename = f"{self.connector_name}_{snapshot_data['trading_pair']}_{date_str}.parquet"
            filepath = self.output_dir / filename
            
            # è½¬æ¢ä¸º DataFrame
            df_new = pd.DataFrame([snapshot_data])
            
            # è¿½åŠ æ¨¡å¼ï¼šå¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯»å–å¹¶åˆå¹¶
            if filepath.exists():
                df_existing = pd.read_parquet(filepath)
                df_combined = pd.concat([df_existing, df_new], ignore_index=True)
            else:
                df_combined = df_new
            
            # ä¿å­˜
            df_combined.to_parquet(
                filepath,
                engine='pyarrow',
                compression='snappy',
                index=False
            )
            
        except Exception as e:
            logger.error(f"Failed to save snapshot: {e}")
            raise
    
    async def cleanup(self, context: TaskContext, result) -> None:
        """ä»»åŠ¡ç»“æŸåçš„æ¸…ç†"""
        try:
            await super().cleanup(context, result)
            logger.info("OrderBookSnapshotTask cleanup completed")
        except Exception as e:
            logger.warning(f"Cleanup error: {e}")


# è¾…åŠ©å‡½æ•°ï¼šè¯»å–å†å²è®¢å•ç°¿æ•°æ®
def load_orderbook_snapshots(
    connector_name: str,
    trading_pair: str,
    start_date: str = None,
    end_date: str = None
) -> pd.DataFrame:
    """
    è¯»å–å†å²è®¢å•ç°¿å¿«ç…§æ•°æ®
    
    Args:
        connector_name: äº¤æ˜“æ‰€åç§°
        trading_pair: äº¤æ˜“å¯¹
        start_date: å¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰
    
    Returns:
        DataFrame with orderbook snapshots
    
    Example:
        >>> df = load_orderbook_snapshots('gate_io', 'IRON-USDT', '20241101', '20241115')
        >>> print(df.head())
    """
    output_dir = data_paths.raw_dir / "orderbook_snapshots"
    
    if not output_dir.exists():
        logger.warning(f"Orderbook snapshots directory not found: {output_dir}")
        return pd.DataFrame()
    
    # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
    pattern = f"{connector_name}_{trading_pair}_*.parquet"
    files = list(output_dir.glob(pattern))
    
    if not files:
        logger.warning(f"No orderbook snapshots found for {connector_name} {trading_pair}")
        return pd.DataFrame()
    
    # è¿‡æ»¤æ—¥æœŸèŒƒå›´
    if start_date or end_date:
        filtered_files = []
        for file in files:
            date_str = file.stem.split('_')[-1]  # æå–æ—¥æœŸéƒ¨åˆ†
            if start_date and date_str < start_date:
                continue
            if end_date and date_str > end_date:
                continue
            filtered_files.append(file)
        files = filtered_files
    
    # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰æ–‡ä»¶
    dfs = []
    for file in sorted(files):
        df = pd.read_parquet(file)
        dfs.append(df)
    
    if not dfs:
        return pd.DataFrame()
    
    combined_df = pd.concat(dfs, ignore_index=True)
    
    # æŒ‰æ—¶é—´æ’åº
    if 'timestamp' in combined_df.columns:
        combined_df = combined_df.sort_values('timestamp')
    
    logger.info(f"Loaded {len(combined_df)} orderbook snapshots from {len(files)} files")
    
    return combined_df


def validate_update_ids(df: pd.DataFrame) -> Dict[str, Any]:
    """
    éªŒè¯è®¢å•ç°¿æ•°æ®çš„ update_id å®Œæ•´æ€§
    
    Args:
        df: è®¢å•ç°¿ DataFrameï¼ˆå¿…é¡»åŒ…å« 'update_id' åˆ—ï¼‰
    
    Returns:
        éªŒè¯æŠ¥å‘Šå­—å…¸ï¼ŒåŒ…å«ï¼š
        - total_records: æ€»è®°å½•æ•°
        - null_count: update_id ä¸ºç©ºçš„è®°å½•æ•°
        - non_increasing: update_id æœªé€’å¢çš„ä½ç½®
        - duplicates: é‡å¤çš„ update_id
        - quality_score: æ•°æ®è´¨é‡è¯„åˆ† (0-100)
    
    Example:
        >>> df = load_orderbook_snapshots('gate_io', 'IRON-USDT')
        >>> report = validate_update_ids(df)
        >>> print(f"è´¨é‡è¯„åˆ†: {report['quality_score']:.1f}/100")
    """
    report = {
        'total_records': len(df),
        'null_count': 0,
        'non_increasing': [],
        'duplicates': [],
        'quality_score': 100.0
    }
    
    if 'update_id' not in df.columns:
        report['error'] = 'No update_id column found'
        report['quality_score'] = 0
        logger.error("âŒ DataFrame does not contain 'update_id' column")
        return report
    
    # æ£€æŸ¥ null å€¼
    null_count = df['update_id'].isna().sum()
    if null_count > 0:
        report['null_count'] = int(null_count)
        report['quality_score'] -= (null_count / len(df)) * 50
        logger.warning(f"âš ï¸ Found {null_count} null update_id values")
    
    # è¿‡æ»¤æœ‰æ•ˆçš„ update_id
    df_valid = df.dropna(subset=['update_id']).copy()
    
    if len(df_valid) < 2:
        logger.warning("âš ï¸ Not enough valid records to validate")
        return report
    
    # æ£€æŸ¥é€’å¢æ€§ï¼ˆGate.io REST API çš„ update_id åº”è¯¥é€’å¢ä½†ä¸ä¸€å®šè¿ç»­ï¼‰
    for i in range(1, len(df_valid)):
        current_id = df_valid.iloc[i]['update_id']
        prev_id = df_valid.iloc[i-1]['update_id']
        
        if current_id <= prev_id:
            issue_type = 'equal' if current_id == prev_id else 'decreasing'
            report['non_increasing'].append({
                'index': i,
                'timestamp': str(df_valid.iloc[i]['timestamp']),
                'prev_id': int(prev_id),
                'current_id': int(current_id),
                'issue': issue_type
            })
    
    # æ£€æŸ¥é‡å¤
    duplicate_mask = df_valid.duplicated(subset=['update_id'], keep=False)
    if duplicate_mask.any():
        duplicates = df_valid[duplicate_mask][['timestamp', 'update_id', 'trading_pair']]
        report['duplicates'] = duplicates.to_dict('records')
        logger.warning(f"âš ï¸ Found {len(duplicates)} duplicate update_id values")
    
    # è®¡ç®—è´¨é‡è¯„åˆ†
    issue_count = len(report['non_increasing']) + len(report['duplicates'])
    if issue_count > 0:
        penalty = min(50, (issue_count / len(df_valid)) * 100)
        report['quality_score'] -= penalty
        logger.warning(f"âš ï¸ Quality score: {report['quality_score']:.1f}/100 ({issue_count} issues)")
    else:
        logger.info(f"âœ… Data quality excellent: {report['quality_score']:.1f}/100")
    
    return report

