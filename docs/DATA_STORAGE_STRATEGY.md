# ğŸ’¾ QuantsLab æ•°æ®å­˜å‚¨ç­–ç•¥è¯¦è§£

## ğŸ“Š å­˜å‚¨æ¶æ„æ€»è§ˆ

QuantsLab ä½¿ç”¨ **æ··åˆå­˜å‚¨ç­–ç•¥**ï¼Œæ ¹æ®æ•°æ®ç±»å‹å’Œç”¨é€”é€‰æ‹©æœ€åˆé€‚çš„å­˜å‚¨æ–¹å¼ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®å­˜å‚¨æ¶æ„                              â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  æ—¶åºæ•°æ®         â”‚        â”‚   å…ƒæ•°æ®/ç»“æ„åŒ–   â”‚          â”‚
â”‚  â”‚  (Time-Series)   â”‚        â”‚   (Metadata)     â”‚          â”‚
â”‚  â”‚                  â”‚        â”‚                  â”‚          â”‚
â”‚  â”‚  â€¢ Kçº¿æ•°æ®        â”‚        â”‚  â€¢ ä»»åŠ¡æ‰§è¡Œå†å²   â”‚          â”‚
â”‚  â”‚  â€¢ äº¤æ˜“æ•°æ®       â”‚        â”‚  â€¢ æ± å­ç­›é€‰ç»“æœ   â”‚          â”‚
â”‚  â”‚  â€¢ èµ„é‡‘è´¹ç‡       â”‚        â”‚  â€¢ é…ç½®ä¿¡æ¯       â”‚          â”‚
â”‚  â”‚                  â”‚        â”‚  â€¢ ç”¨æˆ·æ•°æ®       â”‚          â”‚
â”‚  â”‚  â–¼              â”‚        â”‚  â–¼              â”‚          â”‚
â”‚  â”‚  Parquet Files  â”‚        â”‚  MongoDB        â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—‚ï¸ 1. Parquet æ–‡ä»¶å­˜å‚¨ï¼ˆCLOB æ•°æ®ï¼‰

### å­˜å‚¨å†…å®¹

**ä¸»è¦æ•°æ®ç±»å‹**ï¼š
- âœ… **Kçº¿æ•°æ®** (OHLCV + äº¤æ˜“é‡)
- âœ… **äº¤æ˜“æ•°æ®** (Trades)
- âœ… **èµ„é‡‘è´¹ç‡** (Funding Rates)

### å­˜å‚¨ä½ç½®

```
app/data/cache/candles/
â”œâ”€â”€ binance_perpetual|BTC-USDT|15m.parquet
â”œâ”€â”€ binance_perpetual|BTC-USDT|1h.parquet
â”œâ”€â”€ binance_perpetual|ETH-USDT|15m.parquet
â””â”€â”€ ... (æ›´å¤šäº¤æ˜“å¯¹å’Œæ—¶é—´é—´éš”)
```

**æ–‡ä»¶å‘½åæ ¼å¼**:
```
{connector_name}|{trading_pair}|{interval}.parquet
```

### ä¸ºä»€ä¹ˆç”¨ Parquetï¼Ÿ

#### ä¼˜åŠ¿ âœ…

1. **é«˜æ€§èƒ½å‹ç¼©** (å‹ç¼©ç‡ 80-90%)
   ```python
   # CSV æ–‡ä»¶: 100 MB
   # Parquet æ–‡ä»¶: 10-20 MB
   ```

2. **åˆ—å¼å­˜å‚¨** - å¿«é€ŸæŸ¥è¯¢ç‰¹å®šåˆ—
   ```python
   # åªè¯»å– 'close' åˆ—ï¼Œä¸åŠ è½½æ•´ä¸ªæ•°æ®é›†
   df = pd.read_parquet(file, columns=['timestamp', 'close'])
   ```

3. **ç±»å‹ä¿æŒ** - è‡ªåŠ¨ä¿ç•™æ•°æ®ç±»å‹
   ```python
   # ä¸éœ€è¦é‡å¤æŒ‡å®š dtype
   # æ—¶é—´æˆ³ã€æ•°å­—ç±»å‹è‡ªåŠ¨æ­£ç¡®
   ```

4. **å¿«é€Ÿè¯»å†™** - æ¯” CSV å¿« 10-100 å€
   ```python
   # CSV:     è¯»å– 1GB æ•°æ® â‰ˆ 30 ç§’
   # Parquet: è¯»å– 1GB æ•°æ® â‰ˆ 3 ç§’
   ```

5. **æ”¯æŒåˆ†åŒº** - å¯ä»¥æŒ‰æ—¥æœŸ/äº¤æ˜“å¯¹åˆ†åŒº
   ```python
   # åªè¯»å–ç‰¹å®šæ—¥æœŸçš„æ•°æ®
   df = pd.read_parquet(file, filters=[('date', '>', '2024-01-01')])
   ```

### æ•°æ®æµç¨‹

```python
# 1. ä»äº¤æ˜“æ‰€ API è·å–æ•°æ®
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  äº¤æ˜“æ‰€ API      â”‚ (Binance/OKX/Bybit...)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLOBDataSource  â”‚ 
â”‚ â€¢ get_candles() â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å†…å­˜ç¼“å­˜         â”‚ (_candles_cache)
â”‚ Dict[Tuple, DF] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ dump_candles_cache()
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parquet æ–‡ä»¶     â”‚ (app/data/cache/candles/)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒä»£ç å®ç°

```python
# ä¿å­˜ç¼“å­˜åˆ° Parquet æ–‡ä»¶
def dump_candles_cache(self):
    for key, df in self._candles_cache.items():
        connector, pair, interval = key
        filename = f"{connector}|{pair}|{interval}.parquet"
        filepath = data_paths.get_candles_path(filename)
        
        df.to_parquet(
            filepath,
            engine='pyarrow',
            compression='snappy',  # å¿«é€Ÿå‹ç¼©
            index=True
        )
```

### Parquet æ–‡ä»¶ç»“æ„

```python
# è¯»å–ç¤ºä¾‹
import pandas as pd

df = pd.read_parquet("binance_perpetual|BTC-USDT|15m.parquet")
print(df.head())

# è¾“å‡º:
#                      timestamp       open       high        low      close      volume
# 2024-09-04 00:00:00  1725408000  56789.5  56850.0  56700.0  56820.0  1234.567
# 2024-09-04 00:15:00  1725408900  56820.0  56900.0  56800.0  56880.0  1456.789
# 2024-09-04 00:30:00  1725409800  56880.0  56950.0  56850.0  56920.0  1678.901
```

**åˆ—ç»“æ„**:
- `timestamp` - Unix æ—¶é—´æˆ³
- `open` - å¼€ç›˜ä»·
- `high` - æœ€é«˜ä»·
- `low` - æœ€ä½ä»·
- `close` - æ”¶ç›˜ä»·
- `volume` - æˆäº¤é‡ï¼ˆåŸºç¡€è´§å¸ï¼‰
- `quote_asset_volume` - æˆäº¤é‡ï¼ˆæŠ¥ä»·è´§å¸ï¼‰
- `n_trades` - äº¤æ˜“ç¬”æ•°
- `taker_buy_base_volume` - Takerä¹°å…¥é‡ï¼ˆåŸºç¡€ï¼‰
- `taker_buy_quote_volume` - Takerä¹°å…¥é‡ï¼ˆæŠ¥ä»·ï¼‰

---

## ğŸ—„ï¸ 2. MongoDB å­˜å‚¨ï¼ˆå…ƒæ•°æ®ï¼‰

### å­˜å‚¨å†…å®¹

**ä¸»è¦é›†åˆ**:

#### A. `task_executions` - ä»»åŠ¡æ‰§è¡Œå†å²
```json
{
  "_id": ObjectId("..."),
  "execution_id": "uuid-string",
  "task_name": "candles_downloader",
  "status": "completed",
  "started_at": ISODate("2024-10-04T00:00:00Z"),
  "completed_at": ISODate("2024-10-04T00:10:00Z"),
  "duration_seconds": 600,
  "result_data": {
    "stats": {
      "pairs_processed": 150,
      "candles_downloaded": 432000,
      "errors": 3
    }
  },
  "error_message": null
}
```

#### B. `pools` - DEX æ± å­ç­›é€‰ç»“æœ
```json
{
  "_id": ObjectId("..."),
  "timestamp": ISODate("2024-10-04T00:00:00Z"),
  "execution_id": "uuid-string",
  "network": "solana",
  "trending_pools": [
    {
      "name": "SOL/USDC",
      "address": "pool-address",
      "fdv_usd": 125000,
      "volume_usd_h24": 250000,
      "reserve_in_usd": 80000,
      "volume_liquidity_ratio": 3.125,
      "transactions_h24_buys": 450,
      "transactions_h24_sells": 380
    }
  ],
  "filtered_trending_pools": [...],
  "new_pools": [...],
  "filtered_new_pools": [...]
}
```

#### C. `volume_volatility_screener` - ç­›é€‰å™¨ç»“æœ
```json
{
  "_id": ObjectId("..."),
  "timestamp": ISODate("..."),
  "screener_results": {
    "top_markets": ["BTC-USDT", "ETH-USDT", ...],
    "scores": {...},
    "metrics": {...}
  }
}
```

### ä¸ºä»€ä¹ˆç”¨ MongoDBï¼Ÿ

#### ä¼˜åŠ¿ âœ…

1. **çµæ´»çš„æ–‡æ¡£ç»“æ„** - æ— éœ€é¢„å®šä¹‰ schema
2. **å¿«é€ŸæŸ¥è¯¢** - æ”¯æŒå¤æ‚çš„è¿‡æ»¤å’Œèšåˆ
3. **æ˜“äºæ‰©å±•** - æ·»åŠ æ–°å­—æ®µæ— éœ€è¿ç§»
4. **æ—¶é—´åºåˆ—ä¼˜åŒ–** - æ”¯æŒæ—¶é—´åºåˆ—é›†åˆ
5. **èšåˆç®¡é“** - å¼ºå¤§çš„æ•°æ®åˆ†æèƒ½åŠ›

### ä¸é€‚åˆ MongoDB çš„æ•°æ®

âŒ **ä¸è¦åœ¨ MongoDB å­˜å‚¨**:
- Kçº¿æ•°æ®ï¼ˆå¤ªå¤§ï¼ŒæŸ¥è¯¢æ…¢ï¼‰
- é«˜é¢‘äº¤æ˜“æ•°æ®ï¼ˆå†™å…¥å‹åŠ›å¤§ï¼‰
- éœ€è¦åˆ—å¼åˆ†æçš„æ•°æ®ï¼ˆåˆ†ææ•ˆç‡ä½ï¼‰

---

## ğŸ“‚ 3. å®Œæ•´çš„ç›®å½•ç»“æ„

```
app/data/
â”œâ”€â”€ cache/                          # ç¼“å­˜æ•°æ®
â”‚   â””â”€â”€ candles/                    # âœ… Parquet: Kçº¿æ•°æ®
â”‚       â”œâ”€â”€ binance_perpetual|BTC-USDT|15m.parquet
â”‚       â”œâ”€â”€ binance_perpetual|BTC-USDT|1h.parquet
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ processed/                      # å¤„ç†åçš„æ•°æ®
â”‚   â”œâ”€â”€ backtesting/                # âœ… SQLite/Parquet: å›æµ‹ç»“æœ
â”‚   â”‚   â””â”€â”€ optimization_database.db
â”‚   â””â”€â”€ live_bot_databases/         # âœ… SQLite: å®ç›˜æ•°æ®åº“
â”‚       â””â”€â”€ bot_*.db
â”‚
â””â”€â”€ raw/                            # åŸå§‹æ•°æ®
    â””â”€â”€ ...

MongoDB (quants_lab database):      # âœ… MongoDB: å…ƒæ•°æ®
â”œâ”€â”€ task_executions                 # ä»»åŠ¡æ‰§è¡Œå†å²
â”œâ”€â”€ pools                           # æ± å­ç­›é€‰ç»“æœ
â”œâ”€â”€ volume_volatility_screener      # ç­›é€‰å™¨ç»“æœ
â””â”€â”€ ...å…¶ä»–é›†åˆ
```

---

## ğŸ”„ 4. æ•°æ®è®¿é—®æ¨¡å¼

### è®¿é—® Parquet æ•°æ®ï¼ˆKçº¿ï¼‰

```python
import pandas as pd
from core.data_paths import data_paths

# æ–¹æ³•1: ç›´æ¥è¯»å–æ–‡ä»¶
btc_df = pd.read_parquet(
    data_paths.get_candles_path("binance_perpetual|BTC-USDT|15m.parquet")
)

# æ–¹æ³•2: é€šè¿‡ CLOBDataSourceï¼ˆæ¨èï¼‰
from core.data_sources import CLOBDataSource

clob = CLOBDataSource()

# ä»ç¼“å­˜åŠ è½½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
candles = clob.get_candles_from_cache(
    connector_name="binance_perpetual",
    trading_pair="BTC-USDT",
    interval="15m"
)

# æˆ–è€…è·å–æ•°æ®ï¼ˆè‡ªåŠ¨å¤„ç†ç¼“å­˜ï¼‰
candles = await clob.get_candles(
    connector_name="binance_perpetual",
    trading_pair="BTC-USDT",
    interval="15m",
    start_time=start,
    end_time=end
)

# è®¿é—® DataFrame
df = candles.data
```

### è®¿é—® MongoDB æ•°æ®

```python
from core.database_manager import db_manager

# è·å– MongoDB å®¢æˆ·ç«¯
mongo = await db_manager.get_mongodb_client()

# æŸ¥è¯¢æ± å­æ•°æ®
pools = await mongo.find_documents(
    collection_name="pools",
    query={"network": "solana"},
    sort=[("timestamp", -1)],
    limit=10
)

# æŸ¥è¯¢ä»»åŠ¡æ‰§è¡Œå†å²
executions = await mongo.find_documents(
    collection_name="task_executions",
    query={"task_name": "candles_downloader"},
    sort=[("started_at", -1)],
    limit=5
)
```

---

## ğŸ’¡ 5. æœ€ä½³å®è·µ

### âœ… ä»€ä¹ˆæ—¶å€™ç”¨ Parquet

- æ—¶åºæ•°æ®ï¼ˆKçº¿ã€äº¤æ˜“ã€èµ„é‡‘è´¹ç‡ï¼‰
- éœ€è¦åˆ—å¼åˆ†æçš„æ•°æ®
- æ•°æ®é‡å¤§ï¼ˆ>10MBï¼‰
- éœ€è¦å¿«é€Ÿè¯»å†™
- éœ€è¦é«˜å‹ç¼©ç‡

### âœ… ä»€ä¹ˆæ—¶å€™ç”¨ MongoDB

- å…ƒæ•°æ®ï¼ˆä»»åŠ¡çŠ¶æ€ã€é…ç½®ï¼‰
- ç­›é€‰ç»“æœï¼ˆæ± å­ã€å¸‚åœºï¼‰
- éœ€è¦å¤æ‚æŸ¥è¯¢
- æ•°æ®ç»“æ„çµæ´»
- éœ€è¦å®æ—¶æŸ¥è¯¢

### âœ… ä»€ä¹ˆæ—¶å€™ç”¨ SQLite

- å›æµ‹ç»“æœï¼ˆOptuna ä¼˜åŒ–ï¼‰
- å®ç›˜æœºå™¨äººæ•°æ®åº“
- éœ€è¦å…³ç³»å‹æŸ¥è¯¢
- å•æœºéƒ¨ç½²

---

## ğŸ¯ 6. æ€§èƒ½å¯¹æ¯”

### å­˜å‚¨ 1 å¹´ BTC-USDT 15 åˆ†é’Ÿ Kçº¿æ•°æ®

| æŒ‡æ ‡ | CSV | Parquet | MongoDB |
|------|-----|---------|---------|
| æ–‡ä»¶å¤§å° | 120 MB | 15 MB | 150 MB |
| å†™å…¥æ—¶é—´ | 45 ç§’ | 3 ç§’ | 60 ç§’ |
| è¯»å–æ—¶é—´ | 30 ç§’ | 2 ç§’ | 25 ç§’ |
| æŸ¥è¯¢ç‰¹å®šåˆ— | 30 ç§’ | 0.5 ç§’ | 5 ç§’ |
| å‹ç¼©ç‡ | æ—  | 87.5% | 20% |

**ç»“è®º**: Parquet æ˜¯æ—¶åºæ•°æ®çš„æœ€ä½³é€‰æ‹©ï¼ âœ…

---

## ğŸ” 7. å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆä¸æŠŠ Kçº¿æ•°æ®å­˜åˆ° MongoDBï¼Ÿ

**A:** 
- âŒ å­˜å‚¨æˆæœ¬é«˜ï¼ˆæ— é«˜æ•ˆå‹ç¼©ï¼‰
- âŒ æŸ¥è¯¢æ…¢ï¼ˆä¸æ”¯æŒåˆ—å¼æŸ¥è¯¢ï¼‰
- âŒ å†…å­˜å ç”¨å¤§ï¼ˆéœ€è¦åŠ è½½æ•´ä¸ªæ–‡æ¡£ï¼‰
- âŒ ä¸é€‚åˆæ—¶åºæ•°æ®åˆ†æ

### Q2: Parquet æ–‡ä»¶å¯ä»¥ç›´æ¥ç”¨ Pandas è¯»å–å—ï¼Ÿ

**A:** âœ… å®Œå…¨å¯ä»¥ï¼
```python
import pandas as pd
df = pd.read_parquet("file.parquet")
```

### Q3: å¦‚ä½•å¤‡ä»½æ•°æ®ï¼Ÿ

**A:** 
```bash
# å¤‡ä»½ Parquet æ–‡ä»¶
tar -czf candles_backup.tar.gz app/data/cache/candles/

# å¤‡ä»½ MongoDB
mongodump --uri="mongodb://admin:admin@localhost:27017" --db=quants_lab
```

### Q4: å¯ä»¥æŠŠæ•°æ®è¿ç§»åˆ° MongoDB å—ï¼Ÿ

**A:** æŠ€æœ¯ä¸Šå¯ä»¥ï¼Œä½† **å¼ºçƒˆä¸æ¨è**ï¼š
- ä¼šæ˜¾è‘—é™ä½æ€§èƒ½
- å¢åŠ å­˜å‚¨æˆæœ¬
- å¤±å» Parquet çš„åˆ—å¼æŸ¥è¯¢ä¼˜åŠ¿

### Q5: å¦‚ä½•æ¸…ç†æ—§æ•°æ®ï¼Ÿ

**A:**
```bash
# åˆ é™¤ 30 å¤©å‰çš„æ•°æ®
find app/data/cache/candles/ -name "*.parquet" -mtime +30 -delete

# æˆ–è€…åœ¨ä»»åŠ¡é…ç½®ä¸­è®¾ç½®
config:
  days_data_retention: 30  # åªä¿ç•™ 30 å¤©
```

---

## ğŸ“š 8. ç›¸å…³ä»£ç ä½ç½®

- **Parquet å­˜å‚¨å®ç°**: `core/data_sources/clob.py`
- **æ•°æ®è·¯å¾„ç®¡ç†**: `core/data_paths.py`
- **MongoDB å®¢æˆ·ç«¯**: `core/services/mongodb_client.py`
- **æ•°æ®åº“ç®¡ç†å™¨**: `core/database_manager.py`

---

## ğŸ“ˆ DEX OHLCV Data (GeckoTerminal)

### Storage Format

**Format**: Parquet (same as CEX)

**Naming Convention**: `geckoterminal_{network}|{trading_pair}|{interval}.parquet`

**Storage Location**: `app/data/cache/candles/` (shared with CEX data)

### Schema Compatibility

Matches CEX candles schema with placeholder columns for unsupported fields:

```
timestamp (index, UTC DatetimeIndex)
open, high, low, close, volume          # From GeckoTerminal API
quote_asset_volume = 0                  # Placeholder (not available)
n_trades = 0                            # Placeholder (not available)
taker_buy_base_volume = 0               # Placeholder (not available)
taker_buy_quote_volume = 0              # Placeholder (not available)
```

### Merge Behavior

**Incremental downloads**:
- Fetch overlapping data (last 10 candles)
- Deduplication on timestamp before persistence
- Safe for concurrent access via load â†’ merge â†’ save pattern

**Example**:
```python
from core.services.geckoterminal_ohlcv import load_existing_parquet, merge_and_sort

# Load existing
existing_df = load_existing_parquet(file_path)

# Merge with new data
merged_df = merge_and_sort(existing_df, new_df)

# Save
merged_df.to_parquet(file_path)
```

### Raw Responses (Optional)

**Location**: `app/data/raw/geckoterminal/ohlcv/{network}/`

**Format**: Aggregated JSON per pool

**File**: `{pool_address}_raw.json`

**Structure**:
```json
{
  "responses": [
    {
      "timestamp": "2025-10-12T10:30:00Z",
      "data": {
        "data": {
          "attributes": {
            "ohlcv_list": [[timestamp, o, h, l, c, v], ...]
          }
        },
        "meta": {...}
      }
    }
  ]
}
```

**Use Cases**:
- API debugging
- Data verification
- Historical response tracking
- Not required for normal operation

### File Examples

**DEX Data**:
```
app/data/cache/candles/
â”œâ”€â”€ geckoterminal_base|AERO-USDT|5m.parquet
â”œâ”€â”€ geckoterminal_base|AERO-USDT|15m.parquet
â”œâ”€â”€ geckoterminal_base|AERO-USDT|1h.parquet
â”œâ”€â”€ geckoterminal_base|BRETT-USDT|5m.parquet
â””â”€â”€ ...
```

**CEX Data** (for comparison):
```
app/data/cache/candles/
â”œâ”€â”€ gate_io|AERO-USDT|5m.parquet
â”œâ”€â”€ gate_io|AERO-USDT|15m.parquet
â”œâ”€â”€ gate_io|AERO-USDT|1h.parquet
â”œâ”€â”€ gate_io|BRETT-USDT|5m.parquet
â””â”€â”€ ...
```

**Both can coexist** in the same directory, distinguished by the connector/network prefix.

### Data Validation

**Check for quality**:
```python
import pandas as pd

# Load DEX data
dex_df = pd.read_parquet('app/data/cache/candles/geckoterminal_base|AERO-USDT|5m.parquet')

# 1. Check for duplicates
assert dex_df.index.is_unique, "Duplicated timestamps found"

# 2. Check for NaN
assert not dex_df.isnull().any().any(), "NaN values found"

# 3. Check timestamp continuity
time_diff = dex_df.index.to_series().diff()
expected_diff = pd.Timedelta(minutes=5)
gaps = time_diff[time_diff > expected_diff * 1.5]
if not gaps.empty:
    print(f"Found {len(gaps)} gaps in data")

# 4. Verify schema
expected_cols = ['open', 'high', 'low', 'close', 'volume', 
                 'quote_asset_volume', 'n_trades', 
                 'taker_buy_base_volume', 'taker_buy_quote_volume']
assert all(col in dex_df.columns for col in expected_cols), "Missing columns"

print("âœ“ Data validation passed")
```

### Performance Characteristics

**Read Performance** (same as CEX):
- 5m data, 7 days: ~10MB file, <100ms load time
- 1h data, 30 days: ~5MB file, <50ms load time

**Write Performance**:
- Append 1000 candles: ~50ms
- Merge with dedup: ~100ms

**Query Performance**:
```python
# Filter by time range (fast, uses index)
df[df.index >= '2025-10-01']

# Resample to larger interval (fast)
df.resample('1h').agg({'open': 'first', 'high': 'max', 
                         'low': 'min', 'close': 'last', 'volume': 'sum'})
```

### Storage Comparison

| Feature | CEX (CLOB) | DEX (GeckoTerminal) |
|---------|------------|---------------------|
| Format | Parquet | Parquet |
| Location | `app/data/cache/candles/` | `app/data/cache/candles/` |
| Naming | `{connector}\|{pair}\|{interval}` | `geckoterminal_{network}\|{pair}\|{interval}` |
| OHLCV | âœ“ Full | âœ“ Full |
| Trade Count | âœ“ | âœ— (placeholder 0) |
| Taker Volumes | âœ“ | âœ— (placeholder 0) |
| Quote Volume | âœ“ | âœ— (placeholder 0) |

### Best Practices

1. **Use same intervals** for CEX and DEX data to enable direct comparison
2. **Download DEX data first** using pool mapping, then align CEX downloads
3. **Validate merged data** after incremental updates
4. **Monitor file sizes**: Large files (>100MB) may indicate too long lookback periods
5. **Clean old data** periodically if storage is constrained

---

## ğŸ“ æ€»ç»“

**QuantsLab çš„æ•°æ®å­˜å‚¨å“²å­¦**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  "æ­£ç¡®çš„å·¥å…·åšæ­£ç¡®çš„äº‹"                      â”‚
â”‚                                              â”‚
â”‚  â€¢ Parquet  â†’ æ—¶åºæ•°æ®ï¼ˆæ€§èƒ½ä¼˜å…ˆï¼‰           â”‚
â”‚  â€¢ MongoDB  â†’ å…ƒæ•°æ®ï¼ˆçµæ´»æ€§ä¼˜å…ˆï¼‰           â”‚
â”‚  â€¢ SQLite   â†’ å…³ç³»æ•°æ®ï¼ˆè½»é‡çº§ä¼˜å…ˆï¼‰         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

è¿™ç§æ··åˆå­˜å‚¨ç­–ç•¥ç¡®ä¿äº†ï¼š
- âœ… æœ€ä½³æ€§èƒ½
- âœ… æœ€ä½æˆæœ¬
- âœ… æœ€é«˜çµæ´»æ€§

---

**è®°ä½**: CLOB Kçº¿æ•°æ® = Parquet æ–‡ä»¶ï¼Œä¸åœ¨ MongoDBï¼ğŸ“Šâœ¨

