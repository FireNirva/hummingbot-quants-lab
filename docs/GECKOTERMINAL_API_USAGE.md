# GeckoTerminal API ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£è¯´æ˜é¡¹ç›®ä¸­å¦‚ä½•ä½¿ç”¨ GeckoTerminal APIï¼Œä»¥åŠå¦‚ä½•æ‰©å±•æ”¯æŒå†å² OHLCV Kçº¿æ•°æ®ä¸‹è½½ã€‚

---

## ğŸ“Š å½“å‰ä½¿ç”¨çš„ API Endpoints

### Pool Screener ä½¿ç”¨çš„ API

**æ–‡ä»¶ï¼š** `app/tasks/data_collection/pools_screener.py`

```python
# ç¬¬ 113-114 è¡Œ
top_pools = await self.gt.get_top_pools_by_network(self.network)
new_pools = await self.gt.get_new_pools_by_network(self.network)
```

#### 1. Get Top Pools

**API Endpoint:**
```
GET /networks/{network}/pools
```

**å¯¹åº”çš„ Python æ–¹æ³•:**
```python
await gt.get_top_pools_by_network("base")
```

**è¿”å›æ•°æ®ï¼š**
- è¿”å›æŒ‡å®šç½‘ç»œä¸Šäº¤æ˜“æœ€æ´»è·ƒçš„æ± å­ï¼ˆé»˜è®¤æŒ‰ 24h äº¤æ˜“ç¬”æ•°æ’åºï¼‰
- æœ€å¤šè¿”å› 20 ä¸ªæ± å­ï¼ˆæ¯é¡µï¼‰
- åŒ…å«å®Œæ•´çš„æ± å­ä¿¡æ¯ï¼ˆä»·æ ¼ã€æµåŠ¨æ€§ã€äº¤æ˜“é‡ç­‰ï¼‰

**æ’åºé€‰é¡¹ï¼š**
- `h24_tx_count_desc`ï¼ˆé»˜è®¤ï¼‰- æŒ‰ 24å°æ—¶äº¤æ˜“ç¬”æ•°é™åº
- `h24_volume_usd_desc` - æŒ‰ 24å°æ—¶äº¤æ˜“é‡é™åº

#### 2. Get New Pools

**API Endpoint:**
```
GET /networks/{network}/new_pools
```

**å¯¹åº”çš„ Python æ–¹æ³•:**
```python
await gt.get_new_pools_by_network("base")
```

**è¿”å›æ•°æ®ï¼š**
- è¿”å›æŒ‡å®šç½‘ç»œä¸Šæœ€æ–°åˆ›å»ºçš„æ± å­
- æœ€å¤šè¿”å› 20 ä¸ªæ± å­ï¼ˆæ¯é¡µï¼‰
- æŒ‰åˆ›å»ºæ—¶é—´é™åºæ’åˆ—

### è¿”å›çš„æ± å­æ•°æ®ç»“æ„

æ¯ä¸ªæ± å­åŒ…å«ä»¥ä¸‹å…³é”®å­—æ®µï¼š

```json
{
  "id": "base_uniswap-v3_0x4c36388be6f416a29c8d8eee81c771ce6be14b18",
  "type": "pool",
  "name": "WETH / USDC 0.01%",
  "address": "0x4c36388be6f416a29c8d8eee81c771ce6be14b18",
  "base_token_price_usd": "3260.45",
  "quote_token_price_usd": "1.0",
  "reserve_in_usd": "8326274.123",
  "fdv_usd": "784144285.456",
  "volume_usd_h24": "158589434.234",
  "transactions_h24_buys": 12345,
  "transactions_h24_sells": 11234,
  "price_change_percentage_h1": 0.52,
  "price_change_percentage_h24": 2.34,
  "pool_created_at": "2023-08-15T10:30:00Z",
  "dex_id": "uniswap-v3-base"
}
```

---

## ğŸ“ˆ æ·»åŠ  OHLCV Kçº¿æ•°æ®ä¸‹è½½åŠŸèƒ½

### éœ€è¦ä½¿ç”¨çš„ API

**API Endpoint:**
```
GET /networks/{network}/pools/{pool_address}/ohlcv/{timeframe}
```

### API è¯¦ç»†å‚æ•°

#### å¿…éœ€å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|-----|------|------|------|
| `network` | string | ç½‘ç»œ ID | `base`, `eth`, `solana` |
| `pool_address` | string | æ± å­åœ°å€ | `0x4c36388be6f416a29c8d8eee81c771ce6be14b18` |
| `timeframe` | string | æ—¶é—´æ¡†æ¶ | `day`, `hour`, `minute` |

#### å¯é€‰å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ | å¯é€‰å€¼ | é»˜è®¤å€¼ |
|-----|------|------|-------|-------|
| `aggregate` | string | èšåˆå‘¨æœŸ | day: `1`<br>hour: `1`, `4`, `12`<br>minute: `1`, `5`, `15` | `1` |
| `before_timestamp` | string | è¿”å›æ­¤æ—¶é—´æˆ³ä¹‹å‰çš„æ•°æ® | Unix æ—¶é—´æˆ³ï¼ˆç§’ï¼‰ | å½“å‰æ—¶é—´ |
| `limit` | string | è¿”å›çš„ OHLCV æ•°é‡ | `1-1000` | `100` |
| `currency` | string | è¿”å›ä»·æ ¼çš„è´§å¸å•ä½ | `usd`, `token` | `usd` |
| `token` | string | è¿”å›çš„ä»£å¸ | `base`, `quote`, æˆ–ä»£å¸åœ°å€ | `base` |

### API é™åˆ¶

- â° **å†å²æ•°æ®èŒƒå›´**ï¼šæœ€å¤š 6 ä¸ªæœˆ
- ğŸ“Š **å•æ¬¡æœ€å¤§è¿”å›**ï¼š1000 æ¡ OHLCV æ•°æ®
- ğŸ”„ **æ›´æ–°é¢‘ç‡**ï¼šæ•°æ®ç¼“å­˜ 1 åˆ†é’Ÿ
- ğŸš« **ä¸æ”¯æŒ**ï¼šè¶…è¿‡ 2 ä¸ªä»£å¸çš„æ± å­ï¼ˆå¦‚ Balancer æ± ï¼‰

### è¿”å›æ•°æ®æ ¼å¼

```json
{
  "data": {
    "id": "base_uniswap-v3_0x4c36388be6f416a29c8d8eee81c771ce6be14b18",
    "type": "pool_ohlcv",
    "attributes": {
      "ohlcv_list": [
        [
          1708498800,      // Unix æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
          2955.65,         // Openï¼ˆå¼€ç›˜ä»·ï¼‰
          2955.65,         // Highï¼ˆæœ€é«˜ä»·ï¼‰
          2933.98,         // Lowï¼ˆæœ€ä½ä»·ï¼‰
          2934.24,         // Closeï¼ˆæ”¶ç›˜ä»·ï¼‰
          131664.76        // Volumeï¼ˆäº¤æ˜“é‡ï¼‰
        ],
        // ... æ›´å¤š Kçº¿æ•°æ®
      ]
    }
  },
  "meta": {
    "base": {
      "address": "0xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2",
      "name": "Wrapped Ether",
      "symbol": "WETH"
    },
    "quote": {
      "address": "0x833589fcd6edb6e08f4c7c32d4f71b54bda02913",
      "name": "USD Coin",
      "symbol": "USDC"
    }
  }
}
```

### OHLCV æ•°ç»„æ ¼å¼

æ¯ä¸ª OHLCV æ•°æ®æ˜¯ä¸€ä¸ªåŒ…å« 6 ä¸ªå…ƒç´ çš„æ•°ç»„ï¼š

```python
[
    timestamp,  # 0: Unix æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    open,       # 1: å¼€ç›˜ä»·
    high,       # 2: æœ€é«˜ä»·
    low,        # 3: æœ€ä½ä»·
    close,      # 4: æ”¶ç›˜ä»·
    volume      # 5: äº¤æ˜“é‡
]
```

---

## ğŸ› ï¸ å®ç°å»ºè®®

### æ–¹æ¡ˆ 1ï¼šåˆ›å»ºæ–°çš„ OHLCV ä¸‹è½½ä»»åŠ¡

**æ¨èç†ç”±ï¼š**
- âœ… ä¸ç°æœ‰çš„ candles_downloader_task.py ç»“æ„ä¸€è‡´
- âœ… å¯ä»¥å¤ç”¨ä»»åŠ¡ç³»ç»Ÿçš„è°ƒåº¦ã€é‡è¯•ã€é”™è¯¯å¤„ç†æœºåˆ¶
- âœ… æ•°æ®å­˜å‚¨åœ¨ Parquet æ–‡ä»¶ä¸­ï¼ˆä¸ CLOB æ•°æ®ä¸€è‡´ï¼‰

**å®ç°æ­¥éª¤ï¼š**

#### 1. åˆ›å»ºä»»åŠ¡ç±»

**æ–‡ä»¶ä½ç½®ï¼š** `app/tasks/data_collection/dex_ohlcv_downloader.py`

```python
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
from typing import Dict, Any, List

from geckoterminal_py import GeckoTerminalAsyncClient
from core.tasks import BaseTask, TaskContext
from core.data_paths import DataPaths

class DexOhlcvDownloaderTask(BaseTask):
    """
    ä¸‹è½½ DEX æ± å­çš„å†å² OHLCV æ•°æ®
    
    é…ç½®å‚æ•°ï¼š
    - network: ç½‘ç»œ IDï¼ˆå¦‚ 'base', 'ethereum', 'solana'ï¼‰
    - pools: æ± å­åœ°å€åˆ—è¡¨
    - intervals: æ—¶é—´é—´éš”åˆ—è¡¨ï¼ˆå¦‚ ['1m', '5m', '15m', '1h', '4h', '1d']ï¼‰
    - lookback_days: å›æº¯å¤©æ•°ï¼ˆæœ€å¤š 180 å¤©ï¼‰
    - save_to_parquet: æ˜¯å¦ä¿å­˜ä¸º Parquet æ–‡ä»¶ï¼ˆé»˜è®¤ Trueï¼‰
    """
    
    def __init__(self, config):
        super().__init__(config)
        self.gt = None
        
        # é…ç½®å‚æ•°
        self.network = self.config.config.get("network", "base")
        self.pools = self.config.config.get("pools", [])
        self.intervals = self.config.config.get("intervals", ["15m", "1h", "4h", "1d"])
        self.lookback_days = min(self.config.config.get("lookback_days", 30), 180)
        self.save_to_parquet = self.config.config.get("save_to_parquet", True)
        
        # æ•°æ®è·¯å¾„
        self.data_paths = DataPaths()
        
    async def setup(self, context: TaskContext) -> None:
        """åˆå§‹åŒ– GeckoTerminal å®¢æˆ·ç«¯"""
        await super().setup(context)
        self.gt = GeckoTerminalAsyncClient()
        logging.info(f"DexOhlcvDownloader setup for network: {self.network}")
    
    def _convert_interval_to_api_params(self, interval: str) -> tuple:
        """
        è½¬æ¢é—´éš”æ ¼å¼ä¸º GeckoTerminal API å‚æ•°
        
        Args:
            interval: å¦‚ '15m', '1h', '4h', '1d'
            
        Returns:
            (timeframe, aggregate): å¦‚ ('minute', '15'), ('hour', '4'), ('day', '1')
        """
        interval_map = {
            '1m': ('minute', '1'),
            '5m': ('minute', '5'),
            '15m': ('minute', '15'),
            '1h': ('hour', '1'),
            '4h': ('hour', '4'),
            '12h': ('hour', '12'),
            '1d': ('day', '1'),
        }
        
        if interval not in interval_map:
            raise ValueError(f"Unsupported interval: {interval}. Supported: {list(interval_map.keys())}")
        
        return interval_map[interval]
    
    async def _fetch_ohlcv_chunk(
        self, 
        pool_address: str, 
        timeframe: str, 
        aggregate: str,
        before_timestamp: int = None
    ) -> pd.DataFrame:
        """
        è·å–ä¸€ä¸ª chunk çš„ OHLCV æ•°æ®ï¼ˆæœ€å¤š 1000 æ¡ï¼‰
        
        Returns:
            DataFrame with columns: [timestamp, open, high, low, close, volume]
        """
        try:
            # è°ƒç”¨ GeckoTerminal API
            # æ³¨æ„ï¼šéœ€è¦æ£€æŸ¥ geckoterminal_py åº“æ˜¯å¦æ”¯æŒæ­¤ API
            # å¦‚æœä¸æ”¯æŒï¼Œéœ€è¦ç›´æ¥è°ƒç”¨ HTTP API
            
            params = {
                'aggregate': aggregate,
                'limit': '1000',
                'currency': 'usd'
            }
            
            if before_timestamp:
                params['before_timestamp'] = str(before_timestamp)
            
            # è¿™é‡Œå‡è®¾åº“æ”¯æŒæ­¤æ–¹æ³•ï¼Œå¦‚æœä¸æ”¯æŒéœ€è¦è‡ªå·±å®ç°
            response = await self.gt.get_pool_ohlcv(
                network=self.network,
                pool_address=pool_address,
                timeframe=timeframe,
                **params
            )
            
            # è§£æå“åº”
            ohlcv_list = response['data']['attributes']['ohlcv_list']
            
            # è½¬æ¢ä¸º DataFrame
            df = pd.DataFrame(
                ohlcv_list,
                columns=['timestamp', 'open', 'high', 'low', 'close', 'volume']
            )
            
            # è½¬æ¢æ—¶é—´æˆ³ä¸º datetime
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
            df['date'] = df['timestamp'].dt.date
            
            return df
            
        except Exception as e:
            logging.error(f"Error fetching OHLCV for {pool_address}: {e}")
            return pd.DataFrame()
    
    async def _fetch_full_ohlcv(
        self, 
        pool_address: str, 
        interval: str,
        lookback_days: int
    ) -> pd.DataFrame:
        """
        è·å–å®Œæ•´çš„å†å² OHLCV æ•°æ®ï¼ˆå¯èƒ½éœ€è¦å¤šæ¬¡è¯·æ±‚ï¼‰
        
        Args:
            pool_address: æ± å­åœ°å€
            interval: æ—¶é—´é—´éš”ï¼ˆå¦‚ '15m'ï¼‰
            lookback_days: å›æº¯å¤©æ•°
            
        Returns:
            å®Œæ•´çš„ OHLCV DataFrame
        """
        timeframe, aggregate = self._convert_interval_to_api_params(interval)
        
        all_data = []
        before_timestamp = None
        target_timestamp = int((datetime.now() - timedelta(days=lookback_days)).timestamp())
        
        logging.info(f"Fetching {interval} OHLCV for {pool_address[:10]}... (last {lookback_days} days)")
        
        # å¾ªç¯è·å–æ•°æ®ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡æ—¶é—´æˆ–æ²¡æœ‰æ›´å¤šæ•°æ®
        while True:
            df_chunk = await self._fetch_ohlcv_chunk(
                pool_address=pool_address,
                timeframe=timeframe,
                aggregate=aggregate,
                before_timestamp=before_timestamp
            )
            
            if df_chunk.empty:
                break
            
            all_data.append(df_chunk)
            
            # è·å–æœ€æ—©çš„æ—¶é—´æˆ³
            earliest_timestamp = int(df_chunk['timestamp'].min().timestamp())
            
            # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡æ—¶é—´
            if earliest_timestamp <= target_timestamp:
                break
            
            # æ£€æŸ¥æ˜¯å¦å·²è·å–æ‰€æœ‰å¯ç”¨æ•°æ®ï¼ˆAPI è¿”å›å°‘äº 1000 æ¡ï¼‰
            if len(df_chunk) < 1000:
                break
            
            # è®¾ç½®ä¸‹ä¸€æ¬¡è¯·æ±‚çš„æ—¶é—´æˆ³
            before_timestamp = earliest_timestamp
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(0.5)
        
        if not all_data:
            logging.warning(f"No OHLCV data found for {pool_address}")
            return pd.DataFrame()
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        df_full = pd.concat(all_data, ignore_index=True)
        
        # å»é‡å¹¶æ’åº
        df_full = df_full.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
        
        # è¿‡æ»¤åˆ°ç›®æ ‡æ—¶é—´èŒƒå›´
        df_full = df_full[df_full['timestamp'] >= pd.Timestamp(target_timestamp, unit='s')]
        
        logging.info(f"  âœ“ Fetched {len(df_full)} {interval} candles")
        
        return df_full
    
    async def _save_to_parquet(
        self, 
        df: pd.DataFrame, 
        pool_address: str, 
        interval: str
    ):
        """ä¿å­˜æ•°æ®åˆ° Parquet æ–‡ä»¶"""
        try:
            # åˆ›å»ºç›®å½•ç»“æ„: data/dex_candles/{network}/{pool_address}/
            output_dir = self.data_paths.dex_candles / self.network / pool_address
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # æ–‡ä»¶å: {interval}.parquet
            output_file = output_dir / f"{interval}.parquet"
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
            if output_file.exists():
                df_existing = pd.read_parquet(output_file)
                df = pd.concat([df_existing, df], ignore_index=True)
                df = df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
            
            # ä¿å­˜
            df.to_parquet(output_file, index=False)
            logging.info(f"  âœ“ Saved to {output_file}")
            
        except Exception as e:
            logging.error(f"Error saving to Parquet: {e}")
    
    async def execute(self, context: TaskContext) -> Dict[str, Any]:
        """ä¸»æ‰§è¡Œé€»è¾‘"""
        results = {
            "network": self.network,
            "pools_processed": 0,
            "intervals": {},
            "total_candles": 0
        }
        
        for pool_address in self.pools:
            logging.info(f"\nProcessing pool: {pool_address}")
            
            for interval in self.intervals:
                try:
                    # è·å– OHLCV æ•°æ®
                    df = await self._fetch_full_ohlcv(
                        pool_address=pool_address,
                        interval=interval,
                        lookback_days=self.lookback_days
                    )
                    
                    if df.empty:
                        continue
                    
                    # ä¿å­˜åˆ° Parquet
                    if self.save_to_parquet:
                        await self._save_to_parquet(df, pool_address, interval)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    if interval not in results["intervals"]:
                        results["intervals"][interval] = 0
                    results["intervals"][interval] += len(df)
                    results["total_candles"] += len(df)
                    
                except Exception as e:
                    logging.error(f"Error processing {pool_address} {interval}: {e}")
            
            results["pools_processed"] += 1
        
        logging.info(f"\nâœ“ Download completed: {results['total_candles']} candles from {results['pools_processed']} pools")
        
        return results
```

#### 2. æ·»åŠ æ•°æ®è·¯å¾„å®šä¹‰

**æ–‡ä»¶ï¼š** `core/data_paths.py`

```python
# åœ¨ DataPaths ç±»ä¸­æ·»åŠ 
@property
def dex_candles(self) -> Path:
    """DEX OHLCV candles directory"""
    path = self.data_dir / "dex_candles"
    path.mkdir(parents=True, exist_ok=True)
    return path
```

#### 3. åˆ›å»ºé…ç½®æ–‡ä»¶

**æ–‡ä»¶ï¼š** `config/base_dex_candles_downloader.yml`

```yaml
tasks:
  base_weth_usdc_candles:
    enabled: true
    task_class: app.tasks.data_collection.dex_ohlcv_downloader.DexOhlcvDownloaderTask
    
    schedule:
      type: frequency
      frequency_hours: 1.0  # æ¯å°æ—¶æ›´æ–°ä¸€æ¬¡
      timezone: UTC
    
    max_retries: 3
    retry_delay_seconds: 180
    timeout_seconds: 1800  # 30 åˆ†é’Ÿè¶…æ—¶
    
    config:
      network: "base"
      
      # è¦ä¸‹è½½çš„æ± å­åœ°å€åˆ—è¡¨
      pools:
        # WETH/USDC 0.01% - Uniswap V3
        - "0x4c36388be6f416a29c8d8eee81c771ce6be14b18"
        # WETH/USDC 0.05% - Uniswap V3
        - "0xd0b53d9277642d899df5c87a3966a349a798f224"
        # cbBTC/USDC 0.01%
        - "0x7f0c8b83b935b7c6061235295c6240b8acb40076"
      
      # æ—¶é—´é—´éš”ï¼ˆæ”¯æŒï¼š1m, 5m, 15m, 1h, 4h, 12h, 1dï¼‰
      intervals:
        - "15m"
        - "1h"
        - "4h"
        - "1d"
      
      # å›æº¯å¤©æ•°ï¼ˆæœ€å¤š 180 å¤©ï¼Œå— API é™åˆ¶ï¼‰
      lookback_days: 90
      
      # ä¿å­˜ä¸º Parquet æ–‡ä»¶
      save_to_parquet: true
    
    tags:
      - data_collection
      - dex_candles
      - base
```

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šä¸‹è½½å•ä¸ªæ± å­çš„ Kçº¿æ•°æ®

```python
import asyncio
from geckoterminal_py import GeckoTerminalAsyncClient

async def download_ohlcv():
    gt = GeckoTerminalAsyncClient()
    
    # å‚æ•°
    network = "base"
    pool_address = "0x4c36388be6f416a29c8d8eee81c771ce6be14b18"  # WETH/USDC 0.01%
    
    # è·å– 1 å°æ—¶ Kçº¿æ•°æ®
    response = await gt.get_pool_ohlcv(
        network=network,
        pool_address=pool_address,
        timeframe="hour",
        aggregate="1",
        limit="1000"
    )
    
    ohlcv_list = response['data']['attributes']['ohlcv_list']
    
    # æ‰“å°å‰ 5 æ¡
    for candle in ohlcv_list[:5]:
        timestamp, open_price, high, low, close, volume = candle
        print(f"Time: {timestamp}, O: {open_price}, H: {high}, L: {low}, C: {close}, V: {volume}")

asyncio.run(download_ohlcv())
```

### ç¤ºä¾‹ 2ï¼šè·å–ä¸åŒæ—¶é—´é—´éš”çš„æ•°æ®

```python
intervals = {
    '15m': ('minute', '15'),
    '1h': ('hour', '1'),
    '4h': ('hour', '4'),
    '1d': ('day', '1')
}

for interval_name, (timeframe, aggregate) in intervals.items():
    response = await gt.get_pool_ohlcv(
        network="base",
        pool_address=pool_address,
        timeframe=timeframe,
        aggregate=aggregate,
        limit="100"
    )
    
    count = len(response['data']['attributes']['ohlcv_list'])
    print(f"{interval_name}: {count} candles")
```

### ç¤ºä¾‹ 3ï¼šè·å–å†å²æ•°æ®ï¼ˆåˆ†é¡µï¼‰

```python
async def fetch_historical_data(days_back=30):
    """è·å–è¿‡å» N å¤©çš„æ•°æ®"""
    all_candles = []
    before_timestamp = None
    target_timestamp = int((datetime.now() - timedelta(days=days_back)).timestamp())
    
    while True:
        params = {
            'network': 'base',
            'pool_address': pool_address,
            'timeframe': 'hour',
            'aggregate': '1',
            'limit': '1000'
        }
        
        if before_timestamp:
            params['before_timestamp'] = str(before_timestamp)
        
        response = await gt.get_pool_ohlcv(**params)
        candles = response['data']['attributes']['ohlcv_list']
        
        if not candles:
            break
        
        all_candles.extend(candles)
        
        # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡æ—¶é—´
        earliest_timestamp = candles[-1][0]  # æœ€åä¸€æ¡çš„æ—¶é—´æˆ³
        if earliest_timestamp <= target_timestamp:
            break
        
        before_timestamp = earliest_timestamp
        await asyncio.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    return all_candles
```

---

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### API é™åˆ¶

1. **é€Ÿç‡é™åˆ¶**ï¼š
   - å…è´¹ APIï¼š30 æ¬¡/åˆ†é’Ÿ
   - ä»˜è´¹ APIï¼š500 æ¬¡/åˆ†é’Ÿ
   - å»ºè®®åœ¨è¯·æ±‚ä¹‹é—´æ·»åŠ å»¶è¿Ÿï¼ˆ0.5-1 ç§’ï¼‰

2. **å†å²æ•°æ®é™åˆ¶**ï¼š
   - æœ€å¤š 6 ä¸ªæœˆçš„å†å²æ•°æ®
   - å¦‚éœ€æ›´é•¿æ—¶é—´ï¼Œéœ€è¦å®šæœŸå¢é‡ä¸‹è½½

3. **æ•°æ®ç¼“å­˜**ï¼š
   - API æ•°æ®ç¼“å­˜ 1 åˆ†é’Ÿ
   - é¢‘ç¹è¯·æ±‚ç›¸åŒæ•°æ®ä¸ä¼šè·å¾—å®æ—¶æ›´æ–°

### æ•°æ®è´¨é‡

1. **æ•°æ®å®Œæ•´æ€§**ï¼š
   - DEX æ•°æ®å¯èƒ½å­˜åœ¨ç¼ºå¤±æˆ–ä¸è¿ç»­
   - å»ºè®®éªŒè¯æ—¶é—´æˆ³çš„è¿ç»­æ€§

2. **ä»·æ ¼å‡†ç¡®æ€§**ï¼š
   - DEX ä»·æ ¼å¯èƒ½ä¸ CEX å­˜åœ¨åå·®
   - ä½æµåŠ¨æ€§æ± çš„ä»·æ ¼å¯èƒ½ä¸å‡†ç¡®

3. **äº¤æ˜“é‡**ï¼š
   - 24å°æ—¶äº¤æ˜“é‡æ˜¯æ»šåŠ¨è®¡ç®—çš„
   - ä¸åŒæ—¶é—´æŸ¥è¯¢å¯èƒ½å¾—åˆ°ä¸åŒçš„ç»“æœ

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [GeckoTerminal API å®Œæ•´æ–‡æ¡£](./geckoterminal_api.md)
- [MongoDB æ± å­æ•°æ®å­˜å‚¨](./MONGODB_POOL_STORAGE.md)
- [æ•°æ®å­˜å‚¨ç­–ç•¥](./DATA_STORAGE_STRATEGY.md)
- [Pool Screener é…ç½®](../config/base_pools_production.yml)

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœåœ¨å®ç°è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼š

1. æ£€æŸ¥ `geckoterminal_py` åº“æ˜¯å¦æ”¯æŒ OHLCV API
2. å¦‚æœä¸æ”¯æŒï¼Œå¯ä»¥ä½¿ç”¨ `httpx` æˆ– `aiohttp` ç›´æ¥è°ƒç”¨ HTTP API
3. å‚è€ƒ `core/data_sources/clob.py` ä¸­çš„ CLOB æ•°æ®ä¸‹è½½å®ç°

---

**æœ€åæ›´æ–°ï¼š** 2025-10-05  
**ç»´æŠ¤è€…ï¼š** Alice


