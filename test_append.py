#!/usr/bin/env python3
"""
æµ‹è¯• QuantsLab çš„å¢é‡è¿½åŠ åŠŸèƒ½
"""
import pandas as pd
import asyncio
from pathlib import Path
from datetime import datetime, timezone

# å¯¼å…¥ QuantsLab ç»„ä»¶
from core.data_sources.clob import CLOBDataSource

async def test_incremental_append():
    """æµ‹è¯•å¢é‡è¿½åŠ æ•°æ®åˆ°ç°æœ‰ Parquet æ–‡ä»¶"""
    
    print("="*80)
    print("æµ‹è¯• QuantsLab å¢é‡è¿½åŠ åŠŸèƒ½")
    print("="*80)
    
    # æµ‹è¯•å‚æ•°
    connector = "gate_io"
    trading_pair = "VIRTUAL-USDT"
    interval = "1m"
    
    # Parquet æ–‡ä»¶è·¯å¾„
    cache_dir = Path("app/data/cache/candles")
    parquet_file = cache_dir / f"{connector}|{trading_pair}|{interval}.parquet"
    
    # 1. è¯»å–ç°æœ‰æ•°æ®
    print(f"\nğŸ“Š æ­¥éª¤ 1: è¯»å–ç°æœ‰æ•°æ®")
    print(f"æ–‡ä»¶: {parquet_file}")
    
    if parquet_file.exists():
        existing_df = pd.read_parquet(parquet_file)
        print(f"âœ“ ç°æœ‰æ•°æ®:")
        print(f"  - è¡Œæ•°: {len(existing_df):,}")
        print(f"  - æ—¶é—´èŒƒå›´: {existing_df.index.min()} åˆ° {existing_df.index.max()}")
        
        # è®¡ç®—æ—¶é—´è·¨åº¦
        time_span = existing_df.index.max() - existing_df.index.min()
        print(f"  - è·¨åº¦: {time_span.total_seconds() / 86400:.2f} å¤©")
    else:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {parquet_file}")
        return
    
    # 2. ä½¿ç”¨ CLOBDataSource è·å–æœ€æ–°æ•°æ®
    print(f"\nğŸ“¥ æ­¥éª¤ 2: ä¸‹è½½æœ€æ–°æ•°æ® (0.5å¤© = 720åˆ†é’Ÿ)")
    
    try:
        clob = CLOBDataSource()
        
        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼šä»ç°åœ¨å¾€å‰0.5å¤©
        end_time = int(datetime.now(timezone.utc).timestamp())
        start_time = end_time - int(0.5 * 24 * 60 * 60)  # 0.5å¤©å‰
        
        print(f"  - æ—¶é—´èŒƒå›´: {datetime.fromtimestamp(start_time, timezone.utc)} åˆ° {datetime.fromtimestamp(end_time, timezone.utc)}")
        
        new_candles = await clob.get_candles(
            connector_name=connector,
            trading_pair=trading_pair,
            interval=interval,
            start_time=start_time,
            end_time=end_time
        )
        
        # Candles å¯¹è±¡çš„æ•°æ®åœ¨ .data å±æ€§ä¸­
        new_candles_df = new_candles.data
        
        if new_candles_df is not None and not new_candles_df.empty:
            print(f"âœ“ ä¸‹è½½æˆåŠŸ:")
            print(f"  - è¡Œæ•°: {len(new_candles_df):,}")
            print(f"  - æ—¶é—´èŒƒå›´: {new_candles_df.index.min()} åˆ° {new_candles_df.index.max()}")
        else:
            print(f"âŒ ä¸‹è½½å¤±è´¥æˆ–æ— æ–°æ•°æ®")
            return
            
    except Exception as e:
        print(f"âŒ ä¸‹è½½å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 3. åˆå¹¶æ•°æ®ï¼ˆæ¨¡æ‹Ÿ append è¡Œä¸ºï¼‰
    print(f"\nğŸ”„ æ­¥éª¤ 3: åˆå¹¶æ•°æ®")
    
    # ç»Ÿä¸€æ—¶åŒº (ç¡®ä¿éƒ½æ˜¯ UTC tz-aware)
    if new_candles_df.index.tz is None:
        new_candles_df.index = pd.to_datetime(new_candles_df.index, utc=True)
    if existing_df.index.tz is None:
        existing_df.index = pd.to_datetime(existing_df.index, utc=True)
    
    # åˆå¹¶å¹¶å»é‡
    combined_df = pd.concat([existing_df, new_candles_df])
    combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
    combined_df = combined_df.sort_index()
    
    print(f"âœ“ åˆå¹¶å:")
    print(f"  - æ€»è¡Œæ•°: {len(combined_df):,} (ä¹‹å‰: {len(existing_df):,}, æ–°å¢: {len(combined_df) - len(existing_df):,})")
    print(f"  - æ—¶é—´èŒƒå›´: {combined_df.index.min()} åˆ° {combined_df.index.max()}")
    
    time_span_new = combined_df.index.max() - combined_df.index.min()
    print(f"  - æ–°è·¨åº¦: {time_span_new.total_seconds() / 86400:.2f} å¤©")
    
    # 4. ä¿å­˜ï¼ˆå¯é€‰ï¼‰
    print(f"\nğŸ’¾ æ­¥éª¤ 4: ä¿å­˜ç»“æœ")
    
    # åˆ›å»ºå¤‡ä»½æ–‡ä»¶å
    backup_file = cache_dir / f"{connector}|{trading_pair}|{interval}.backup.parquet"
    
    print(f"  - å¤‡ä»½åŸæ–‡ä»¶: {backup_file}")
    existing_df.to_parquet(backup_file)
    
    print(f"  - ä¿å­˜åˆå¹¶æ•°æ®: {parquet_file}")
    combined_df.to_parquet(parquet_file)
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
    print(f"\nğŸ“ˆ ç»“æœæ€»ç»“:")
    print(f"  - åŸå§‹æ•°æ®: {len(existing_df):,} è¡Œ")
    print(f"  - æ–°ä¸‹è½½: {len(new_candles_df):,} è¡Œ")
    print(f"  - åˆå¹¶å: {len(combined_df):,} è¡Œ")
    print(f"  - å®é™…æ–°å¢: {len(combined_df) - len(existing_df):,} è¡Œ")
    print(f"  - æ—¶é—´æ‰©å±•: {time_span.total_seconds() / 86400:.2f} å¤© â†’ {time_span_new.total_seconds() / 86400:.2f} å¤©")


if __name__ == "__main__":
    asyncio.run(test_incremental_append())

