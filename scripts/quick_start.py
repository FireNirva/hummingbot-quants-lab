#!/usr/bin/env python3
"""
OKXæ–‡æ¡£çˆ¬è™« - å¿«é€Ÿå¼€å§‹
"""

import asyncio
import sys
from pathlib import Path
from okx_docs_crawler_improved import ImprovedOKXDocsCrawler, ImprovedCrawlConfig

async def quick_start():
    """å¿«é€Ÿå¼€å§‹çˆ¬å–OKXæ–‡æ¡£"""
    print("ğŸš€ OKXæ–‡æ¡£çˆ¬è™« - å¿«é€Ÿå¼€å§‹")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    try:
        from selenium import webdriver
        from fake_useragent import UserAgent
        print("âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡")
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·è¿è¡Œ: pip install selenium fake-useragent")
        return False
    
    # é…ç½®çˆ¬è™«
    config = ImprovedCrawlConfig(
        start_url="https://web3.okx.com/zh-hans/build/dev-docs/",
        output_dir="data/okx_docs",
        max_concurrent=1,
        delay_range=(5, 8),
        use_selenium=True,
        headless=True,
        page_load_timeout=30
    )
    
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"ğŸŒ å¼€å§‹URL: {config.start_url}")
    print(f"â±ï¸ å»¶è¿ŸèŒƒå›´: {config.delay_range}ç§’")
    print(f"ğŸ¤– ä½¿ç”¨Selenium: {config.use_selenium}")
    print()
    
    try:
        async with ImprovedOKXDocsCrawler(config) as crawler:
            print("ğŸ”„ å¼€å§‹çˆ¬å–...")
            await crawler.crawl()
            
        print("ğŸ‰ çˆ¬å–å®Œæˆ!")
        
        # æ˜¾ç¤ºç»“æœ
        output_dir = Path(config.output_dir)
        if output_dir.exists():
            md_files = list(output_dir.rglob("*.md"))
            json_files = list(output_dir.rglob("*.json"))
            
            print(f"\nğŸ“Š çˆ¬å–ç»“æœ:")
            print(f"   - Markdownæ–‡ä»¶: {len(md_files)} ä¸ª")
            print(f"   - JSONæ–‡ä»¶: {len(json_files)} ä¸ª")
            print(f"   - è¾“å‡ºç›®å½•: {output_dir}")
            
            # æ˜¾ç¤ºéƒ¨åˆ†æ–‡ä»¶
            if md_files:
                print(f"\nğŸ“„ ç”Ÿæˆçš„æ–‡æ¡£ (å‰5ä¸ª):")
                for i, file in enumerate(md_files[:5], 1):
                    size = file.stat().st_size
                    print(f"   {i}. {file.name} ({size} bytes)")
                    
                if len(md_files) > 5:
                    print(f"   ... è¿˜æœ‰ {len(md_files) - 5} ä¸ªæ–‡ä»¶")
                    
            # æ˜¾ç¤ºç»Ÿè®¡æ–‡ä»¶
            stats_file = output_dir / "crawl_stats.json"
            if stats_file.exists():
                print(f"\nğŸ“ˆ è¯¦ç»†ç»Ÿè®¡: {stats_file}")
                
            report_file = output_dir / "crawl_report.md"
            if report_file.exists():
                print(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        print("\nğŸ› ï¸ æ•…éšœæ’é™¤å»ºè®®:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. ç¡®ä¿Chromeæµè§ˆå™¨å·²å®‰è£…")
        print("3. è¿è¡Œ: python test_improved_crawler.py")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("æ¬¢è¿ä½¿ç”¨OKXæ–‡æ¡£çˆ¬è™«!")
    print("è¿™ä¸ªå·¥å…·å°†å¸®åŠ©ä½ çˆ¬å–OKX DEX APIçš„å®Œæ•´æ–‡æ¡£")
    print()
    
    # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
    response = input("æ˜¯å¦å¼€å§‹çˆ¬å–? (y/n): ").lower().strip()
    if response not in ['y', 'yes', 'æ˜¯', '']:
        print("å·²å–æ¶ˆ")
        return
    
    # è¿è¡Œçˆ¬è™«
    success = asyncio.run(quick_start())
    
    if success:
        print("\nğŸ¯ ä¸‹ä¸€æ­¥:")
        print("1. æŸ¥çœ‹ç”Ÿæˆçš„æ–‡æ¡£: data/okx_docs/")
        print("2. é˜…è¯»ä½¿ç”¨æŒ‡å—: README_ä½¿ç”¨æŒ‡å—.md")
        print("3. ç›‘æ§è¿›åº¦: python monitor_crawl.py")
    else:
        print("\nâ“ éœ€è¦å¸®åŠ©?")
        print("1. æŸ¥çœ‹ä½¿ç”¨æŒ‡å—: README_ä½¿ç”¨æŒ‡å—.md")
        print("2. è¿è¡Œæµ‹è¯•: python test_improved_crawler.py")

if __name__ == "__main__":
    main() 