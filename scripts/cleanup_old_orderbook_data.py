#!/usr/bin/env python3
"""
è®¢å•ç°¿æ•°æ®æ¸…ç†è„šæœ¬

ç”¨é€”ï¼š
- è‡ªåŠ¨æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„è®¢å•ç°¿æ•°æ®
- é‡Šæ”¾ç£ç›˜ç©ºé—´
- æ”¯æŒå¹²è¿è¡Œæ¨¡å¼ï¼ˆé¢„è§ˆä½†ä¸åˆ é™¤ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    # å¹²è¿è¡Œï¼ˆé¢„è§ˆè¦åˆ é™¤çš„æ–‡ä»¶ï¼‰
    python scripts/cleanup_old_orderbook_data.py --days 7 --dry-run
    
    # å®é™…åˆ é™¤
    python scripts/cleanup_old_orderbook_data.py --days 7
    
    # è®¾ç½®ä¸ºå®šæ—¶ä»»åŠ¡ï¼ˆæ¯å¤©å‡Œæ™¨2ç‚¹æ¸…ç†è¶…è¿‡7å¤©çš„æ•°æ®ï¼‰
    0 2 * * * cd /path/to/quants-lab && python scripts/cleanup_old_orderbook_data.py --days 7 >> logs/cleanup.log 2>&1
"""

import argparse
import logging
import shutil
import sys
from datetime import datetime, timedelta, timezone
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.data_paths import data_paths

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_date_from_filename(filename: str) -> datetime:
    """ä»æ–‡ä»¶åè§£ææ—¥æœŸ
    
    æ–‡ä»¶åæ ¼å¼: gate_io_IRON_USDT_20241112.parquet
    """
    try:
        # æå–æ—¥æœŸéƒ¨åˆ†ï¼ˆæœ€åä¸€ä¸ªä¸‹åˆ’çº¿åï¼Œ.parquet å‰ï¼‰
        date_str = filename.split('_')[-1].replace('.parquet', '')
        return datetime.strptime(date_str, '%Y%m%d').replace(tzinfo=timezone.utc)
    except Exception as e:
        logger.warning(f"æ— æ³•è§£ææ–‡ä»¶åæ—¥æœŸ: {filename}, é”™è¯¯: {e}")
        return None


def get_disk_usage(path: Path) -> dict:
    """è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µ"""
    total, used, free = shutil.disk_usage(path)
    return {
        'total_gb': total / (2**30),
        'used_gb': used / (2**30),
        'free_gb': free / (2**30),
        'used_percent': (used / total) * 100
    }


def cleanup_old_files(days_to_keep: int, dry_run: bool = True) -> dict:
    """æ¸…ç†æ—§æ–‡ä»¶
    
    Args:
        days_to_keep: ä¿ç•™æœ€è¿‘Nå¤©çš„æ•°æ®
        dry_run: å¦‚æœä¸ºTrueï¼Œåªé¢„è§ˆä¸åˆ é™¤
        
    Returns:
        æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
    """
    orderbook_dir = data_paths.raw_dir / "orderbook_snapshots"
    
    if not orderbook_dir.exists():
        logger.error(f"è®¢å•ç°¿æ•°æ®ç›®å½•ä¸å­˜åœ¨: {orderbook_dir}")
        return {'error': 'directory_not_found'}
    
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_to_keep)
    
    logger.info(f"{'[å¹²è¿è¡Œ] ' if dry_run else ''}å¼€å§‹æ¸…ç†è®¢å•ç°¿æ•°æ®...")
    logger.info(f"ä¿ç•™ç­–ç•¥: ä¿ç•™æœ€è¿‘ {days_to_keep} å¤©çš„æ•°æ®")
    logger.info(f"åˆ é™¤æ—¥æœŸ: {cutoff_date.strftime('%Y-%m-%d')} ä¹‹å‰")
    logger.info(f"æ•°æ®ç›®å½•: {orderbook_dir}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_files': 0,
        'old_files': 0,
        'deleted_files': 0,
        'skipped_files': 0,
        'total_size_mb': 0,
        'freed_space_mb': 0,
        'errors': 0,
        'deleted_file_list': [],
        'error_file_list': []
    }
    
    # éå†æ‰€æœ‰ parquet æ–‡ä»¶
    for file_path in orderbook_dir.glob("*.parquet"):
        stats['total_files'] += 1
        
        # è§£ææ—¥æœŸ
        file_date = parse_date_from_filename(file_path.name)
        
        if file_date is None:
            stats['skipped_files'] += 1
            stats['error_file_list'].append((file_path.name, 'failed_to_parse_date'))
            continue
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size_mb = file_path.stat().st_size / (1024 * 1024)
        stats['total_size_mb'] += file_size_mb
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤
        if file_date < cutoff_date:
            stats['old_files'] += 1
            
            if dry_run:
                logger.info(f"[å°†åˆ é™¤] {file_path.name} ({file_size_mb:.2f} MB, {file_date.strftime('%Y-%m-%d')})")
                stats['freed_space_mb'] += file_size_mb
                stats['deleted_file_list'].append((file_path.name, file_size_mb, file_date))
            else:
                try:
                    file_path.unlink()
                    logger.info(f"[å·²åˆ é™¤] {file_path.name} ({file_size_mb:.2f} MB)")
                    stats['deleted_files'] += 1
                    stats['freed_space_mb'] += file_size_mb
                    stats['deleted_file_list'].append((file_path.name, file_size_mb, file_date))
                except Exception as e:
                    logger.error(f"åˆ é™¤å¤±è´¥: {file_path.name}, é”™è¯¯: {e}")
                    stats['errors'] += 1
                    stats['error_file_list'].append((file_path.name, str(e)))
    
    return stats


def print_summary(stats: dict, disk_before: dict, disk_after: dict = None, dry_run: bool = True):
    """æ‰“å°æ¸…ç†æ‘˜è¦"""
    print("\n" + "=" * 80)
    print(f"ğŸ—‘ï¸  è®¢å•ç°¿æ•°æ®æ¸…ç†{'é¢„è§ˆ' if dry_run else ''}æŠ¥å‘Š - {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}")
    print("=" * 80)
    
    # æ–‡ä»¶ç»Ÿè®¡
    print(f"\nğŸ“Š æ–‡ä»¶ç»Ÿè®¡:")
    print(f"   â€¢ æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"   â€¢ æ—§æ–‡ä»¶æ•°: {stats['old_files']}")
    
    if dry_run:
        print(f"   â€¢ å°†åˆ é™¤: {stats['old_files']} ä¸ªæ–‡ä»¶")
    else:
        print(f"   â€¢ âœ… å·²åˆ é™¤: {stats['deleted_files']} ä¸ªæ–‡ä»¶")
        if stats['errors'] > 0:
            print(f"   â€¢ âŒ åˆ é™¤å¤±è´¥: {stats['errors']} ä¸ªæ–‡ä»¶")
    
    if stats['skipped_files'] > 0:
        print(f"   â€¢ âš ï¸  è·³è¿‡: {stats['skipped_files']} ä¸ªæ–‡ä»¶ï¼ˆæ— æ³•è§£ææ—¥æœŸï¼‰")
    
    # ç©ºé—´ç»Ÿè®¡
    print(f"\nğŸ’¾ ç©ºé—´ç»Ÿè®¡:")
    print(f"   â€¢ æ•°æ®ç›®å½•æ€»å¤§å°: {stats['total_size_mb']:.2f} MB ({stats['total_size_mb']/1024:.2f} GB)")
    print(f"   â€¢ {'å°†é‡Šæ”¾' if dry_run else 'å·²é‡Šæ”¾'}ç©ºé—´: {stats['freed_space_mb']:.2f} MB ({stats['freed_space_mb']/1024:.2f} GB)")
    
    if stats['old_files'] > 0:
        print(f"   â€¢ å¹³å‡æ–‡ä»¶å¤§å°: {stats['freed_space_mb']/stats['old_files']:.2f} MB")
    
    # ç£ç›˜ä½¿ç”¨æƒ…å†µ
    print(f"\nğŸ’¿ ç£ç›˜ä½¿ç”¨æƒ…å†µ:")
    print(f"   æ¸…ç†å‰:")
    print(f"      â€¢ æ€»ç©ºé—´: {disk_before['total_gb']:.1f} GB")
    print(f"      â€¢ å·²ä½¿ç”¨: {disk_before['used_gb']:.1f} GB ({disk_before['used_percent']:.1f}%)")
    print(f"      â€¢ å¯ç”¨: {disk_before['free_gb']:.1f} GB")
    
    if disk_after:
        print(f"   æ¸…ç†å:")
        print(f"      â€¢ å¯ç”¨: {disk_after['free_gb']:.1f} GB")
        print(f"      â€¢ é‡Šæ”¾: {disk_after['free_gb'] - disk_before['free_gb']:.2f} GB")
    else:
        print(f"   é¢„è®¡æ¸…ç†å:")
        print(f"      â€¢ å¯ç”¨: {disk_before['free_gb'] + stats['freed_space_mb']/1024:.1f} GB")
    
    # è¯¦ç»†åˆ—è¡¨ï¼ˆä»…æ˜¾ç¤ºå‰10ä¸ªï¼‰
    if stats['deleted_file_list']:
        print(f"\n{'ğŸ” å°†åˆ é™¤çš„æ–‡ä»¶' if dry_run else 'âœ… å·²åˆ é™¤çš„æ–‡ä»¶'} (å‰10ä¸ª):")
        for i, (filename, size_mb, file_date) in enumerate(stats['deleted_file_list'][:10], 1):
            print(f"   {i:2d}. {filename:50s} {size_mb:8.2f} MB  {file_date.strftime('%Y-%m-%d')}")
        
        if len(stats['deleted_file_list']) > 10:
            print(f"   ... è¿˜æœ‰ {len(stats['deleted_file_list']) - 10} ä¸ªæ–‡ä»¶")
    
    # é”™è¯¯åˆ—è¡¨
    if stats['error_file_list']:
        print(f"\nâŒ é”™è¯¯åˆ—è¡¨:")
        for filename, error in stats['error_file_list']:
            print(f"   â€¢ {filename}: {error}")
    
    # å»ºè®®
    if dry_run and stats['old_files'] > 0:
        print(f"\nğŸ’¡ å»ºè®®:")
        print(f"   ç¡®è®¤æ— è¯¯åï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤æ‰§è¡Œå®é™…åˆ é™¤:")
        print(f"   python scripts/cleanup_old_orderbook_data.py --days {args.days}")
    elif not dry_run and stats['deleted_files'] > 0:
        print(f"\nâœ… æ¸…ç†å®Œæˆï¼")
        print(f"   é‡Šæ”¾äº† {stats['freed_space_mb']/1024:.2f} GB ç£ç›˜ç©ºé—´")
    elif stats['old_files'] == 0:
        print(f"\nâœ… æ— éœ€æ¸…ç†")
        print(f"   æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ä¿ç•™æœŸå†…")
    
    print("\n" + "=" * 80 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ¸…ç†æ—§çš„è®¢å•ç°¿æ•°æ®æ–‡ä»¶',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    # é¢„è§ˆè¦åˆ é™¤çš„æ–‡ä»¶ï¼ˆä¸å®é™…åˆ é™¤ï¼‰
    python scripts/cleanup_old_orderbook_data.py --days 7 --dry-run
    
    # åˆ é™¤è¶…è¿‡7å¤©çš„æ•°æ®
    python scripts/cleanup_old_orderbook_data.py --days 7
    
    # åˆ é™¤è¶…è¿‡14å¤©çš„æ•°æ®
    python scripts/cleanup_old_orderbook_data.py --days 14
    
    # æŸ¥çœ‹å¸®åŠ©
    python scripts/cleanup_old_orderbook_data.py --help
        """
    )
    
    parser.add_argument(
        '--days',
        type=int,
        default=7,
        help='ä¿ç•™æœ€è¿‘Nå¤©çš„æ•°æ®ï¼ˆé»˜è®¤: 7ï¼‰'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='å¹²è¿è¡Œæ¨¡å¼ï¼šé¢„è§ˆè¦åˆ é™¤çš„æ–‡ä»¶ä½†ä¸å®é™…åˆ é™¤'
    )
    
    global args
    args = parser.parse_args()
    
    # è·å–æ¸…ç†å‰çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ
    orderbook_dir = data_paths.raw_dir / "orderbook_snapshots"
    disk_before = get_disk_usage(orderbook_dir)
    
    # æ‰§è¡Œæ¸…ç†
    stats = cleanup_old_files(days_to_keep=args.days, dry_run=args.dry_run)
    
    if 'error' in stats:
        logger.error(f"æ¸…ç†å¤±è´¥: {stats['error']}")
        sys.exit(1)
    
    # è·å–æ¸…ç†åçš„ç£ç›˜ä½¿ç”¨æƒ…å†µ
    disk_after = None if args.dry_run else get_disk_usage(orderbook_dir)
    
    # æ‰“å°æ‘˜è¦
    print_summary(stats, disk_before, disk_after, dry_run=args.dry_run)
    
    # è¿”å›çŠ¶æ€ç 
    if stats['errors'] > 0:
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    main()

