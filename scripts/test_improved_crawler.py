#!/usr/bin/env python3
"""
æµ‹è¯•æ”¹è¿›çš„OKXæ–‡æ¡£çˆ¬è™«
"""

import asyncio
import sys
from pathlib import Path
from okx_docs_crawler_improved import ImprovedOKXDocsCrawler, ImprovedCrawlConfig

async def test_single_page():
    """æµ‹è¯•å•ä¸ªé¡µé¢çš„çˆ¬å–"""
    print("ğŸ§ª æµ‹è¯•æ”¹è¿›çš„OKXæ–‡æ¡£çˆ¬è™«...")
    
    # é…ç½®æµ‹è¯•
    config = ImprovedCrawlConfig(
        start_url="https://web3.okx.com/zh-hans/build/dev-docs/",
        output_dir="data/okx_docs_test",
        max_concurrent=1,
        delay_range=(3, 5),
        use_selenium=True,
        headless=True,
        page_load_timeout=30
    )
    
    try:
        async with ImprovedOKXDocsCrawler(config) as crawler:
            print(f"ğŸ“„ æµ‹è¯•çˆ¬å–å•ä¸ªé¡µé¢: {config.start_url}")
            
            # çˆ¬å–å•ä¸ªé¡µé¢
            new_links = await crawler.crawl_url(config.start_url)
            
            print(f"âœ… æˆåŠŸçˆ¬å–é¡µé¢")
            print(f"ğŸ“Š å‘ç°æ–°é“¾æ¥: {len(new_links)} ä¸ª")
            print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {config.output_dir}")
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
            output_path = Path(config.output_dir)
            if output_path.exists():
                files = list(output_path.rglob("*.md"))
                print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶: {len(files)} ä¸ª")
                
                for file in files[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ–‡ä»¶
                    print(f"   - {file}")
                    
                # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡ä»¶çš„å†…å®¹é¢„è§ˆ
                if files:
                    first_file = files[0]
                    content = first_file.read_text(encoding='utf-8')
                    print(f"\nğŸ“– æ–‡ä»¶å†…å®¹é¢„è§ˆ ({first_file.name}):")
                    print("=" * 50)
                    print(content[:500] + "..." if len(content) > 500 else content)
                    print("=" * 50)
            
            print(f"\nğŸ‰ æµ‹è¯•å®Œæˆ!")
            return True
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_content_extraction():
    """æµ‹è¯•å†…å®¹æå–åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•å†…å®¹æå–åŠŸèƒ½...")
    
    config = ImprovedCrawlConfig()
    crawler = ImprovedOKXDocsCrawler(config)
    
    # æµ‹è¯•HTMLå†…å®¹
    test_html = """
    <html>
    <head><title>ä»€ä¹ˆæ˜¯ DEX API | æ¦‚è§ˆ | é¦–é¡µ | DEX API | DEX API æ–‡æ¡£ | æ¬§æ˜“</title></head>
    <body>
        <div class="routes_content__fnVIZ">
            <div class="routes_md__xWlGF">
                <h1>ä»€ä¹ˆæ˜¯ DEX API</h1>
                <h2>ä»‹ç»</h2>
                <p>æ¬¢è¿æ¥åˆ° OKX DEX å¼€å‘è€…æ–‡æ¡£ã€‚</p>
                <p>OKX DEX æ˜¯ä¸€ç«™å¼å¤šé“¾è·¨é“¾èšåˆäº¤æ˜“å¹³å°ã€‚</p>
            </div>
        </div>
        <div class="index_table-of-content__dpmyB">
            <a href="/link1">æ¦‚è§ˆ</a>
            <a href="/link2">å¼€å§‹</a>
        </div>
    </body>
    </html>
    """
    
    content = crawler.extract_content_from_react(test_html, "https://test.com")
    
    print(f"ğŸ“ æå–çš„æ ‡é¢˜: {content['title']}")
    print(f"ğŸ“Š å­—æ•°ç»Ÿè®¡: {content['word_count']}")
    print(f"ğŸ”— å¯¼èˆªé“¾æ¥: {len(content['navigation']['sidebar_links'])}")
    print(f"ğŸ“„ æ–‡æœ¬å†…å®¹é¢„è§ˆ: {content['text'][:100]}...")
    
    if content['word_count'] > 0:
        print("âœ… å†…å®¹æå–æµ‹è¯•é€šè¿‡")
        return True
    else:
        print("âŒ å†…å®¹æå–æµ‹è¯•å¤±è´¥")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ”¹è¿›çš„OKXæ–‡æ¡£çˆ¬è™«")
    
    # æ£€æŸ¥ä¾èµ–
    try:
        from selenium import webdriver
        from fake_useragent import UserAgent
        print("âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡")
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·è¿è¡Œ: pip install selenium fake-useragent")
        return False
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("å†…å®¹æå–åŠŸèƒ½", test_content_extraction),
        ("å•é¡µé¢çˆ¬å–", test_single_page),
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n{'='*50}")
        print(f"ğŸ§ª è¿è¡Œæµ‹è¯•: {test_name}")
        print(f"{'='*50}")
        
        try:
            result = await test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ æµ‹è¯• {test_name} å‡ºé”™: {e}")
            results.append((test_name, False))
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
    print(f"\n{'='*50}")
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"{'='*50}")
    
    passed = 0
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! çˆ¬è™«å·²å‡†å¤‡å°±ç»ª")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("python okx_docs_crawler_improved.py")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
    
    return passed == len(results)

if __name__ == "__main__":
    asyncio.run(main()) 