# OKXæ–‡æ¡£çˆ¬è™«ä½¿ç”¨æŒ‡å—

## ğŸ¯ åŠŸèƒ½ç‰¹ç‚¹

âœ… **è§£å†³React SPAé—®é¢˜**ï¼šä½¿ç”¨Seleniumç­‰å¾…JavaScriptæ¸²æŸ“å®Œæˆ  
âœ… **æ™ºèƒ½å†…å®¹æå–**ï¼šé’ˆå¯¹OKXæ–‡æ¡£ç»“æ„ä¼˜åŒ–çš„å†…å®¹æå–å™¨  
âœ… **åæ£€æµ‹æœºåˆ¶**ï¼šéšæœºUser-Agentã€å»¶è¿Ÿã€æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸º  
âœ… **ç»“æ„åŒ–è¾“å‡º**ï¼šåŒæ—¶ç”ŸæˆMarkdownå’ŒJSONæ ¼å¼  
âœ… **å®Œæ•´å¯¼èˆªä¿¡æ¯**ï¼šä¿ç•™é¢åŒ…å±‘ã€ä¾§è¾¹æ ã€ç›®å½•ç»“æ„  

## ğŸš€ åŸºæœ¬ä½¿ç”¨

### 1. å¿«é€Ÿå¼€å§‹

```bash
# è¿è¡Œçˆ¬è™«ï¼ˆåå°æ¨¡å¼ï¼‰
python okx_docs_crawler_improved.py

# ç›‘æ§è¿›åº¦
python monitor_crawl.py
```

### 2. è‡ªå®šä¹‰é…ç½®è¿è¡Œ

```python
from okx_docs_crawler_improved import ImprovedOKXDocsCrawler, ImprovedCrawlConfig
import asyncio

# è‡ªå®šä¹‰é…ç½®
config = ImprovedCrawlConfig(
    start_url="https://web3.okx.com/zh-hans/build/dev-docs/",
    output_dir="data/my_okx_docs",
    max_concurrent=1,        # å¹¶å‘æ•°ï¼ˆå»ºè®®ä¿æŒ1ï¼‰
    delay_range=(8, 15),     # å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰
    use_selenium=True,       # ä½¿ç”¨Selenium
    headless=True,           # æ— å¤´æ¨¡å¼
    page_load_timeout=30     # é¡µé¢åŠ è½½è¶…æ—¶
)

# è¿è¡Œçˆ¬è™«
async def main():
    async with ImprovedOKXDocsCrawler(config) as crawler:
        await crawler.crawl()

asyncio.run(main())
```

## ğŸ“ è¾“å‡ºç»“æ„

```
data/okx_docs/
â”œâ”€â”€ zh-hans/
â”‚   â””â”€â”€ build/
â”‚       â”œâ”€â”€ dev-docs.md          # é¦–é¡µæ–‡æ¡£
â”‚       â”œâ”€â”€ dev-docs.json        # é¦–é¡µå…ƒæ•°æ®
â”‚       â””â”€â”€ dev-docs/
â”‚           â””â”€â”€ dex-api/
â”‚               â”œâ”€â”€ dex-trade-api-introduction.md
â”‚               â”œâ”€â”€ dex-trade-api-introduction.json
â”‚               â”œâ”€â”€ dex-market-api-introduction.md
â”‚               â””â”€â”€ ...
â”œâ”€â”€ crawl_stats.json             # çˆ¬å–ç»Ÿè®¡
â””â”€â”€ crawl_report.md              # è¯¦ç»†æŠ¥å‘Š
```

## ğŸ“„ æ–‡ä»¶æ ¼å¼è¯´æ˜

### Markdownæ–‡ä»¶ (.md)
```markdown
# é¡µé¢æ ‡é¢˜

**URL:** åŸå§‹é“¾æ¥
**æŠ“å–æ—¶é—´:** 2025-05-27 00:04:00
**å­—æ•°:** 88

## å¯¼èˆªè·¯å¾„
DEX API > é¦–é¡µ > ä»€ä¹ˆæ˜¯ DEX API

## ç›®å½•
- ä»‹ç»
- ä¸ºä»€ä¹ˆé€‰æ‹© OKX DEX APIï¼Ÿ

---

[æ–‡æ¡£æ­£æ–‡å†…å®¹]

---

<details>
<summary>åŸå§‹HTMLå†…å®¹</summary>
[åŸå§‹HTMLä»£ç ]
</details>

<details>
<summary>å¯¼èˆªä¿¡æ¯</summary>
[JSONæ ¼å¼çš„å¯¼èˆªæ•°æ®]
</details>
```

### JSONæ–‡ä»¶ (.json)
```json
{
  "title": "é¡µé¢æ ‡é¢˜",
  "text": "çº¯æ–‡æœ¬å†…å®¹",
  "html": "åŸå§‹HTML",
  "url": "é¡µé¢URL",
  "navigation": {
    "breadcrumbs": ["é¢åŒ…å±‘å¯¼èˆª"],
    "sidebar_links": ["ä¾§è¾¹æ é“¾æ¥"],
    "toc": ["ç›®å½•é¡¹"]
  },
  "word_count": 88,
  "extract_time": "2025-05-27 00:04:00"
}
```

## âš™ï¸ é…ç½®å‚æ•°è¯¦è§£

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `start_url` | OKXæ–‡æ¡£é¦–é¡µ | å¼€å§‹çˆ¬å–çš„URL |
| `output_dir` | `data/okx_docs` | è¾“å‡ºç›®å½• |
| `max_concurrent` | 1 | å¹¶å‘æ•°ï¼ˆå»ºè®®ä¿æŒ1é¿å…è¢«æ£€æµ‹ï¼‰ |
| `delay_range` | (5, 10) | è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ |
| `timeout` | 60 | HTTPè¶…æ—¶æ—¶é—´ |
| `use_selenium` | True | æ˜¯å¦ä½¿ç”¨Selenium |
| `headless` | True | æ˜¯å¦æ— å¤´æ¨¡å¼ |
| `page_load_timeout` | 30 | é¡µé¢åŠ è½½è¶…æ—¶ |

## ğŸ”§ å¸¸ç”¨å‘½ä»¤

### æ£€æŸ¥çˆ¬å–çŠ¶æ€
```bash
# æŸ¥çœ‹å·²çˆ¬å–çš„æ–‡ä»¶æ•°é‡
find data/okx_docs -name "*.md" | wc -l

# æŸ¥çœ‹æœ€æ–°çˆ¬å–çš„æ–‡ä»¶
ls -lt data/okx_docs/zh-hans/build/dev-docs/dex-api/ | head -5

# æŸ¥çœ‹çˆ¬å–ç»Ÿè®¡
cat data/okx_docs/crawl_stats.json
```

### é‡æ–°å¼€å§‹çˆ¬å–
```bash
# æ¸…ç†ä¹‹å‰çš„ç»“æœ
rm -rf data/okx_docs

# é‡æ–°è¿è¡Œ
python okx_docs_crawler_improved.py
```

### åªçˆ¬å–ç‰¹å®šé¡µé¢
```python
# æµ‹è¯•å•ä¸ªé¡µé¢
python test_improved_crawler.py
```

## ğŸ“Š ç›‘æ§å’Œç®¡ç†

### å®æ—¶ç›‘æ§
```bash
python monitor_crawl.py
```

ç›‘æ§è¾“å‡ºç¤ºä¾‹ï¼š
```
ğŸ” ç›‘æ§OKXæ–‡æ¡£çˆ¬å–è¿›åº¦...
ğŸ“ è¾“å‡ºç›®å½•: data\okx_docs
============================================================
â° 00:09:39 | ğŸ“„ å·²çˆ¬å–: 37 é¡µé¢ | â±ï¸ ç”¨æ—¶: 285ç§’
ğŸ“ æœ€æ–°æ–‡ä»¶: dex-approve-transaction.md
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
   - æˆåŠŸ: 37 é¡µé¢
   - å¤±è´¥: 2 é¡µé¢
   - æˆåŠŸç‡: 94.9%
```

### æŸ¥çœ‹çˆ¬å–æŠ¥å‘Š
```bash
# æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š
cat data/okx_docs/crawl_report.md

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
python -c "import json; print(json.dumps(json.load(open('data/okx_docs/crawl_stats.json')), indent=2, ensure_ascii=False))"
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **Chromeé©±åŠ¨é—®é¢˜**
   ```bash
   # å®‰è£…/æ›´æ–°Chromeé©±åŠ¨
   pip install --upgrade selenium
   # æˆ–æ‰‹åŠ¨ä¸‹è½½å¯¹åº”ç‰ˆæœ¬çš„chromedriver
   ```

2. **å†…å­˜ä¸è¶³**
   ```python
   # é™ä½å¹¶å‘æ•°
   config.max_concurrent = 1
   # å¢åŠ å»¶è¿Ÿ
   config.delay_range = (10, 20)
   ```

3. **è¢«åçˆ¬è™«æ£€æµ‹**
   ```python
   # å¢åŠ å»¶è¿Ÿ
   config.delay_range = (15, 30)
   # ä½¿ç”¨ä»£ç†ï¼ˆéœ€è¦é¢å¤–é…ç½®ï¼‰
   ```

4. **é¡µé¢åŠ è½½è¶…æ—¶**
   ```python
   # å¢åŠ è¶…æ—¶æ—¶é—´
   config.page_load_timeout = 60
   config.timeout = 120
   ```

### è°ƒè¯•æ¨¡å¼

```bash
# è¿è¡Œè°ƒè¯•è„šæœ¬
python debug_okx_page.py

# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
python okx_docs_crawler_improved.py 2>&1 | tee crawl.log
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **ç½‘ç»œç¯å¢ƒ**ï¼šä½¿ç”¨ç¨³å®šçš„ç½‘ç»œè¿æ¥
2. **ç³»ç»Ÿèµ„æº**ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼ˆå»ºè®®4GB+ï¼‰
3. **å¹¶å‘æ§åˆ¶**ï¼šä¿æŒ`max_concurrent=1`é¿å…è¢«æ£€æµ‹
4. **å»¶è¿Ÿè®¾ç½®**ï¼šæ ¹æ®ç½‘ç»œæƒ…å†µè°ƒæ•´`delay_range`
5. **å®šæœŸæ¸…ç†**ï¼šåˆ é™¤ä¸éœ€è¦çš„ä¸´æ—¶æ–‡ä»¶

## ğŸ¯ ä½¿ç”¨æŠ€å·§

### 1. æ‰¹é‡å¤„ç†
```bash
# çˆ¬å–å®Œæˆåæ‰¹é‡è½¬æ¢æ ¼å¼
for file in data/okx_docs/**/*.md; do
    echo "å¤„ç†: $file"
    # ä½ çš„å¤„ç†é€»è¾‘
done
```

### 2. å†…å®¹æœç´¢
```bash
# åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­æœç´¢å…³é”®è¯
grep -r "API" data/okx_docs/ --include="*.md"

# ç»Ÿè®¡è¯é¢‘
cat data/okx_docs/**/*.md | tr ' ' '\n' | sort | uniq -c | sort -nr | head -20
```

### 3. æ•°æ®åˆ†æ
```python
import json
from pathlib import Path

# åˆ†æçˆ¬å–çš„æ•°æ®
docs_dir = Path("data/okx_docs")
json_files = list(docs_dir.rglob("*.json"))

total_words = 0
for file in json_files:
    if file.name != "crawl_stats.json":
        data = json.loads(file.read_text(encoding='utf-8'))
        total_words += data.get('word_count', 0)

print(f"æ€»å­—æ•°: {total_words}")
print(f"å¹³å‡æ¯é¡µå­—æ•°: {total_words / len(json_files):.0f}")
```

## ğŸ“ æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼š
1. æŸ¥çœ‹ `crawl_stats.json` ä¸­çš„é”™è¯¯ä¿¡æ¯
2. æ£€æŸ¥ `crawl_report.md` ä¸­çš„è¯¦ç»†æŠ¥å‘Š
3. è¿è¡Œ `python test_improved_crawler.py` è¿›è¡Œè¯Šæ–­

---

**æ³¨æ„**ï¼šè¯·éµå®ˆç½‘ç«™çš„robots.txtå’Œä½¿ç”¨æ¡æ¬¾ï¼Œåˆç†ä½¿ç”¨çˆ¬è™«å·¥å…·ã€‚ 